{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85beaa1",
   "metadata": {},
   "source": [
    "The following is a short sample demonstration about how to use [tokenizers](https://pypi.org/project/tokenizers/) package to tokenise a very small set of ADR terms (or words) into tokens, then encode ADR terms with token IDs, followed by a final decoding of these token IDs back into the corresponding ADR terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd095a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ] at 2025-05-15 16:24:27.156234\n"
     ]
    }
   ],
   "source": [
    "#from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "import sys, datetime\n",
    "print(f\"Python version used: {sys.version} at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7138af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample normalizers code to \"normalise\" texts\n",
    "# somehow the normalizer code is not quite working yet... text data in and the same text data out...\n",
    "\n",
    "# from tokenizers.models import BPE, WordLevel, WordPiece\n",
    "# from tokenizers import Tokenizer, normalizers\n",
    "# from tokenizers.normalizers import StripAccents, Sequence, Replace\n",
    "\n",
    "# BPE - byte pair encoding\n",
    "# bpe_tokenizer = Tokenizer(BPE())\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "# bpe_tokenizer.normalizer = normalizers.Sequence([StripAccents()])\n",
    "## normalizer seems to be set already even though code seems not right within the normalizers.Sequence() (?)\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "\n",
    "# sentences = ['abdominal_pain', 'Höw aRę ŸõŪ dÔįñg?']\n",
    "\n",
    "# normalized_sentences = [bpe_tokenizer.normalizer.normalize_str(s) for s in sentences]\n",
    "# normalized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ac0c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hot_flush^,': 26,\n",
       " 'gastroesophageal_reflux_disease^,': 21,\n",
       " 'fever^,': 19,\n",
       " 'joint_swelling^,': 32,\n",
       " 'anemia^,': 10,\n",
       " '[CLS]': 2,\n",
       " '[MASK]': 4,\n",
       " 'anaphylaxis(pm)': 9,\n",
       " 'oropharyngeal_pain^,': 38,\n",
       " 'rhinitis^,': 44,\n",
       " 'RTI^^,': 7,\n",
       " 'hepatic_cirrhosis(pm),': 25,\n",
       " 'neutropenia(pm),': 37,\n",
       " 'peripheral_edema^,': 41,\n",
       " 'headache^^,': 22,\n",
       " 'angioedema(pm),': 11,\n",
       " 'thrombocytopenia(pm),': 49,\n",
       " 'pruritus^,': 42,\n",
       " 'hypersensitivity_reaction^,': 27,\n",
       " 'nasopharyngitis^,': 36,\n",
       " 'diarrhea^,': 15,\n",
       " '[PAD]': 1,\n",
       " 'hypotension^,': 28,\n",
       " 'orthostatic_hypotension^,': 39,\n",
       " 'idiopathic_pulmonary_fibrosis^,': 29,\n",
       " 'dermatitis(pm),': 14,\n",
       " 'hemoglobin_decreased^^,': 24,\n",
       " 'influenza_like_illness^,': 30,\n",
       " 'flushing^,': 20,\n",
       " '[UNK]': 0,\n",
       " 'hematocrit_decreased^,': 23,\n",
       " 'erythema^,': 18,\n",
       " 'abnormal_LFT^^,': 8,\n",
       " 'DRESS(pm),': 6,\n",
       " 'liver_failure(pm),': 34,\n",
       " 'chest_pain^,': 5,\n",
       " 'sinusitis^,': 46,\n",
       " 'unstable_angina^,': 50,\n",
       " 'nasal_congestion^,': 35,\n",
       " 'leukopenia(pm),': 33,\n",
       " 'syncope^,': 48,\n",
       " 'vertigo^,': 51,\n",
       " 'sinus_congestion^,': 45,\n",
       " 'arthralgia^,': 12,\n",
       " 'epistaxis^,': 17,\n",
       " 'jaundice(pm),': 31,\n",
       " 'palpitation^,': 40,\n",
       " 'blurred_vision^,': 13,\n",
       " 'edema^^,': 16,\n",
       " '[SEP]': 3,\n",
       " 'rash(pm),': 43,\n",
       " 'sperm_count_decreased^^,': 47}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text data from one of CYP3A4 substrates - bosenten's ADRs \n",
    "# since ADRs data are preprocessed a bit more than raw texts found elsewhere, decided to go straight to create a tokenizer\n",
    "data = [\"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"]\n",
    "\n",
    "#UNK_TOKEN = '[UNK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "\n",
    "# have not yet taken into account of unknown words or padding token\n",
    "tokenizer = Tokenizer(models.WordLevel())\n",
    "\n",
    "# below link explains about how to add special tokens e.g. unknown tokens to take into account diff. scenarios\n",
    "# https://huggingface.co/learn/llm-course/chapter6/8?fw=pt#building-a-wordpiece-tokenizer-from-scratch\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=100000, special_tokens=special_tokens)\n",
    "\n",
    "# training tokenizer \n",
    "# specify iterator - pass through iterator a sequence of sequences in the data via using map() function to apply split()\n",
    "# and trainer\n",
    "tokenizer.train_from_iterator(map(lambda x: x.split(), data), trainer=trainer)\n",
    "\n",
    "tokenizer.get_vocab()\n",
    "# returns the indices of each token in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c664ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal_LFT^^,', 'headache^^,', 'RTI^^,', 'hemoglobin_decreased^^,', 'sperm_count_decreased^^,', 'edema^^,', 'hepatic_cirrhosis(pm),', 'liver_failure(pm),', 'jaundice(pm),', 'syncope^,', 'sinusitis^,', 'nasal_congestion^,', 'sinus_congestion^,', 'rhinitis^,', 'oropharyngeal_pain^,', 'epistaxis^,', 'nasopharyngitis^,', 'idiopathic_pulmonary_fibrosis^,', 'anemia^,', 'hematocrit_decreased^,', 'thrombocytopenia(pm),', 'neutropenia(pm),', 'leukopenia(pm),', 'flushing^,', 'hypotension^,', 'palpitation^,', 'orthostatic_hypotension^,', 'unstable_angina^,', 'hot_flush^,', 'gastroesophageal_reflux_disease^,', 'diarrhea^,', 'pruritus^,', 'erythema^,', 'angioedema(pm),', 'DRESS(pm),', 'rash(pm),', 'dermatitis(pm),', 'arthralgia^,', 'joint_swelling^,', 'blurred_vision^,', 'chest_pain^,', 'peripheral_edema^,', 'influenza_like_illness^,', 'vertigo^,', 'fever^,', 'chest_pain^,', 'hypersensitivity_reaction^,', 'anaphylaxis(pm)']\n"
     ]
    }
   ],
   "source": [
    "# using str.split() but punctuations such as commas are not stripped/splitted\n",
    "for t in data:\n",
    "    print(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dba5eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('abnormal_LFT', (0, 12)),\n",
       "  ('^^,', (12, 15)),\n",
       "  ('headache', (16, 24)),\n",
       "  ('^^,', (24, 27)),\n",
       "  ('RTI', (28, 31)),\n",
       "  ('^^,', (31, 34)),\n",
       "  ('hemoglobin_decreased', (35, 55)),\n",
       "  ('^^,', (55, 58)),\n",
       "  ('sperm_count_decreased', (59, 80)),\n",
       "  ('^^,', (80, 83)),\n",
       "  ('edema', (84, 89)),\n",
       "  ('^^,', (89, 92)),\n",
       "  ('hepatic_cirrhosis', (93, 110)),\n",
       "  ('(', (110, 111)),\n",
       "  ('pm', (111, 113)),\n",
       "  ('),', (113, 115)),\n",
       "  ('liver_failure', (116, 129)),\n",
       "  ('(', (129, 130)),\n",
       "  ('pm', (130, 132)),\n",
       "  ('),', (132, 134)),\n",
       "  ('jaundice', (135, 143)),\n",
       "  ('(', (143, 144)),\n",
       "  ('pm', (144, 146)),\n",
       "  ('),', (146, 148)),\n",
       "  ('syncope', (149, 156)),\n",
       "  ('^,', (156, 158)),\n",
       "  ('sinusitis', (159, 168)),\n",
       "  ('^,', (168, 170)),\n",
       "  ('nasal_congestion', (171, 187)),\n",
       "  ('^,', (187, 189)),\n",
       "  ('sinus_congestion', (190, 206)),\n",
       "  ('^,', (206, 208)),\n",
       "  ('rhinitis', (209, 217)),\n",
       "  ('^,', (217, 219)),\n",
       "  ('oropharyngeal_pain', (220, 238)),\n",
       "  ('^,', (238, 240)),\n",
       "  ('epistaxis', (241, 250)),\n",
       "  ('^,', (250, 252)),\n",
       "  ('nasopharyngitis', (253, 268)),\n",
       "  ('^,', (268, 270)),\n",
       "  ('idiopathic_pulmonary_fibrosis', (271, 300)),\n",
       "  ('^,', (300, 302)),\n",
       "  ('anemia', (303, 309)),\n",
       "  ('^,', (309, 311)),\n",
       "  ('hematocrit_decreased', (312, 332)),\n",
       "  ('^,', (332, 334)),\n",
       "  ('thrombocytopenia', (335, 351)),\n",
       "  ('(', (351, 352)),\n",
       "  ('pm', (352, 354)),\n",
       "  ('),', (354, 356)),\n",
       "  ('neutropenia', (357, 368)),\n",
       "  ('(', (368, 369)),\n",
       "  ('pm', (369, 371)),\n",
       "  ('),', (371, 373)),\n",
       "  ('leukopenia', (374, 384)),\n",
       "  ('(', (384, 385)),\n",
       "  ('pm', (385, 387)),\n",
       "  ('),', (387, 389)),\n",
       "  ('flushing', (390, 398)),\n",
       "  ('^,', (398, 400)),\n",
       "  ('hypotension', (401, 412)),\n",
       "  ('^,', (412, 414)),\n",
       "  ('palpitation', (415, 426)),\n",
       "  ('^,', (426, 428)),\n",
       "  ('orthostatic_hypotension', (429, 452)),\n",
       "  ('^,', (452, 454)),\n",
       "  ('unstable_angina', (455, 470)),\n",
       "  ('^,', (470, 472)),\n",
       "  ('hot_flush', (473, 482)),\n",
       "  ('^,', (482, 484)),\n",
       "  ('gastroesophageal_reflux_disease', (485, 516)),\n",
       "  ('^,', (516, 518)),\n",
       "  ('diarrhea', (519, 527)),\n",
       "  ('^,', (527, 529)),\n",
       "  ('pruritus', (530, 538)),\n",
       "  ('^,', (538, 540)),\n",
       "  ('erythema', (541, 549)),\n",
       "  ('^,', (549, 551)),\n",
       "  ('angioedema', (552, 562)),\n",
       "  ('(', (562, 563)),\n",
       "  ('pm', (563, 565)),\n",
       "  ('),', (565, 567)),\n",
       "  ('DRESS', (568, 573)),\n",
       "  ('(', (573, 574)),\n",
       "  ('pm', (574, 576)),\n",
       "  ('),', (576, 578)),\n",
       "  ('rash', (579, 583)),\n",
       "  ('(', (583, 584)),\n",
       "  ('pm', (584, 586)),\n",
       "  ('),', (586, 588)),\n",
       "  ('dermatitis', (589, 599)),\n",
       "  ('(', (599, 600)),\n",
       "  ('pm', (600, 602)),\n",
       "  ('),', (602, 604)),\n",
       "  ('arthralgia', (605, 615)),\n",
       "  ('^,', (615, 617)),\n",
       "  ('joint_swelling', (618, 632)),\n",
       "  ('^,', (632, 634)),\n",
       "  ('blurred_vision', (635, 649)),\n",
       "  ('^,', (649, 651)),\n",
       "  ('chest_pain', (652, 662)),\n",
       "  ('^,', (662, 664)),\n",
       "  ('peripheral_edema', (665, 681)),\n",
       "  ('^,', (681, 683)),\n",
       "  ('influenza_like_illness', (684, 706)),\n",
       "  ('^,', (706, 708)),\n",
       "  ('vertigo', (709, 716)),\n",
       "  ('^,', (716, 718)),\n",
       "  ('fever', (719, 724)),\n",
       "  ('^,', (724, 726)),\n",
       "  ('chest_pain', (727, 737)),\n",
       "  ('^,', (737, 739)),\n",
       "  ('hypersensitivity_reaction', (740, 765)),\n",
       "  ('^,', (765, 767)),\n",
       "  ('anaphylaxis', (768, 779)),\n",
       "  ('(', (779, 780)),\n",
       "  ('pm', (780, 782)),\n",
       "  (')', (782, 783))]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pre_tokenizer will split at white spaces and remove punctuations, and set tokens for each word and each punctuation\n",
    "pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "split_data = [pre_tokenizer.pre_tokenize_str(t) for t in data]\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbd1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, token: [UNK]\n",
      "ID: 1, token: [PAD]\n",
      "ID: 2, token: [CLS]\n",
      "ID: 3, token: [SEP]\n",
      "ID: 4, token: [MASK]\n",
      "ID: 5, token: chest_pain^,\n",
      "ID: 6, token: DRESS(pm),\n",
      "ID: 7, token: RTI^^,\n",
      "ID: 8, token: abnormal_LFT^^,\n",
      "ID: 9, token: anaphylaxis(pm)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'ID: {i}, token: {tokenizer.id_to_token(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717cc361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique tokens (words)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d57a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable padding\n",
    "# need to find out if pad_id is always necessary e.g. pad_id = tokenizer.token_to_id(PAD_TOKEN)\n",
    "tokenizer.enable_padding(pad_token=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa19af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 5]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode('vertigo^,', 'chest_pain^,')\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fac569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vertigo^, chest_pain^,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([51, 5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
