{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a003f9",
   "metadata": {},
   "source": [
    "Merging 5-1 & 5-2 notebooks - to be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85beaa1",
   "metadata": {},
   "source": [
    "The following is a short sample demonstration about how to use [tokenizers](https://pypi.org/project/tokenizers/) package to tokenise a very small set of ADR terms (or words) into tokens, then encode ADR terms with token IDs, followed by a final decoding of these token IDs back into the corresponding ADR terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058484e5",
   "metadata": {},
   "source": [
    "Below is a short demonstration on using AutoTokenizer from [transformers](https://pypi.org/project/transformers/) package to tokenise/encode and decode one line of texts. A separate notebook (5-1_Tokenizers_to_tokenise_texts.ipynb) has been prepared to show how to use tokenizers package to tokenise/encode and decode text data or ADR terms.\n",
    "\n",
    "Reference links: \n",
    "* https://huggingface.co/learn/llm-course/chapter2/4?fw=pt#tokenization\n",
    "* https://huggingface.co/docs/datasets/use_dataset#tokenize-text\n",
    "\n",
    "Plan\n",
    "* initial small goal is to try using tokenizer.decode(), so building a tokenization/tokenizer model first\n",
    "* tokenisation appears to be workable through two different packages e.g. tokenizers or transformers package\n",
    "* the idea is that this text data tokenisation part can be added to a larger DNN model to decode ADRs output later (subject to further idea changes... may try a small NER classification model first, see 6_NER_tk_inhibitors.ipynb)\n",
    "\n",
    "* trying HuggingFace's transformers package:\n",
    "1. set up tokenizer model that will tokenize the ADRs/words\n",
    "2. apply tokenizer.decode() function to each tensor row/sequence (via using list comprehension)\n",
    "3. use sample code snippet below to decode tensors: \n",
    "e.g. decoded = [tokenizer.decode(x) for x in adrs_ts]\n",
    "- the code will iterate through each row/sequence of tensors and apply the decode() method \n",
    "which'll transform the numerical IDs back into human-readable texts/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd095a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ] at 2025-05-15 16:24:27.156234\n"
     ]
    }
   ],
   "source": [
    "#from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "import sys, datetime\n",
    "print(f\"Python version used: {sys.version} at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7138af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample normalizers code to \"normalise\" texts\n",
    "# somehow the normalizer code is not quite working yet... text data in and the same text data out...\n",
    "\n",
    "# from tokenizers.models import BPE, WordLevel, WordPiece\n",
    "# from tokenizers import Tokenizer, normalizers\n",
    "# from tokenizers.normalizers import StripAccents, Sequence, Replace\n",
    "\n",
    "# BPE - byte pair encoding\n",
    "# bpe_tokenizer = Tokenizer(BPE())\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "# bpe_tokenizer.normalizer = normalizers.Sequence([StripAccents()])\n",
    "## normalizer seems to be set already even though code seems not right within the normalizers.Sequence() (?)\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "\n",
    "# sentences = ['abdominal_pain', 'Höw aRę ŸõŪ dÔįñg?']\n",
    "\n",
    "# normalized_sentences = [bpe_tokenizer.normalizer.normalize_str(s) for s in sentences]\n",
    "# normalized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ac0c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hot_flush^,': 26,\n",
       " 'gastroesophageal_reflux_disease^,': 21,\n",
       " 'fever^,': 19,\n",
       " 'joint_swelling^,': 32,\n",
       " 'anemia^,': 10,\n",
       " '[CLS]': 2,\n",
       " '[MASK]': 4,\n",
       " 'anaphylaxis(pm)': 9,\n",
       " 'oropharyngeal_pain^,': 38,\n",
       " 'rhinitis^,': 44,\n",
       " 'RTI^^,': 7,\n",
       " 'hepatic_cirrhosis(pm),': 25,\n",
       " 'neutropenia(pm),': 37,\n",
       " 'peripheral_edema^,': 41,\n",
       " 'headache^^,': 22,\n",
       " 'angioedema(pm),': 11,\n",
       " 'thrombocytopenia(pm),': 49,\n",
       " 'pruritus^,': 42,\n",
       " 'hypersensitivity_reaction^,': 27,\n",
       " 'nasopharyngitis^,': 36,\n",
       " 'diarrhea^,': 15,\n",
       " '[PAD]': 1,\n",
       " 'hypotension^,': 28,\n",
       " 'orthostatic_hypotension^,': 39,\n",
       " 'idiopathic_pulmonary_fibrosis^,': 29,\n",
       " 'dermatitis(pm),': 14,\n",
       " 'hemoglobin_decreased^^,': 24,\n",
       " 'influenza_like_illness^,': 30,\n",
       " 'flushing^,': 20,\n",
       " '[UNK]': 0,\n",
       " 'hematocrit_decreased^,': 23,\n",
       " 'erythema^,': 18,\n",
       " 'abnormal_LFT^^,': 8,\n",
       " 'DRESS(pm),': 6,\n",
       " 'liver_failure(pm),': 34,\n",
       " 'chest_pain^,': 5,\n",
       " 'sinusitis^,': 46,\n",
       " 'unstable_angina^,': 50,\n",
       " 'nasal_congestion^,': 35,\n",
       " 'leukopenia(pm),': 33,\n",
       " 'syncope^,': 48,\n",
       " 'vertigo^,': 51,\n",
       " 'sinus_congestion^,': 45,\n",
       " 'arthralgia^,': 12,\n",
       " 'epistaxis^,': 17,\n",
       " 'jaundice(pm),': 31,\n",
       " 'palpitation^,': 40,\n",
       " 'blurred_vision^,': 13,\n",
       " 'edema^^,': 16,\n",
       " '[SEP]': 3,\n",
       " 'rash(pm),': 43,\n",
       " 'sperm_count_decreased^^,': 47}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text data from one of CYP3A4 substrates - bosenten's ADRs \n",
    "# since ADRs data are preprocessed a bit more than raw texts found elsewhere, decided to go straight to create a tokenizer\n",
    "data = [\"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"]\n",
    "\n",
    "#UNK_TOKEN = '[UNK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "\n",
    "# have not yet taken into account of unknown words or padding token\n",
    "tokenizer = Tokenizer(models.WordLevel())\n",
    "\n",
    "# below link explains about how to add special tokens e.g. unknown tokens to take into account diff. scenarios\n",
    "# https://huggingface.co/learn/llm-course/chapter6/8?fw=pt#building-a-wordpiece-tokenizer-from-scratch\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=100000, special_tokens=special_tokens)\n",
    "\n",
    "# training tokenizer \n",
    "# specify iterator - pass through iterator a sequence of sequences in the data via using map() function to apply split()\n",
    "# and trainer\n",
    "tokenizer.train_from_iterator(map(lambda x: x.split(), data), trainer=trainer)\n",
    "\n",
    "tokenizer.get_vocab()\n",
    "# returns the indices of each token in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c664ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal_LFT^^,', 'headache^^,', 'RTI^^,', 'hemoglobin_decreased^^,', 'sperm_count_decreased^^,', 'edema^^,', 'hepatic_cirrhosis(pm),', 'liver_failure(pm),', 'jaundice(pm),', 'syncope^,', 'sinusitis^,', 'nasal_congestion^,', 'sinus_congestion^,', 'rhinitis^,', 'oropharyngeal_pain^,', 'epistaxis^,', 'nasopharyngitis^,', 'idiopathic_pulmonary_fibrosis^,', 'anemia^,', 'hematocrit_decreased^,', 'thrombocytopenia(pm),', 'neutropenia(pm),', 'leukopenia(pm),', 'flushing^,', 'hypotension^,', 'palpitation^,', 'orthostatic_hypotension^,', 'unstable_angina^,', 'hot_flush^,', 'gastroesophageal_reflux_disease^,', 'diarrhea^,', 'pruritus^,', 'erythema^,', 'angioedema(pm),', 'DRESS(pm),', 'rash(pm),', 'dermatitis(pm),', 'arthralgia^,', 'joint_swelling^,', 'blurred_vision^,', 'chest_pain^,', 'peripheral_edema^,', 'influenza_like_illness^,', 'vertigo^,', 'fever^,', 'chest_pain^,', 'hypersensitivity_reaction^,', 'anaphylaxis(pm)']\n"
     ]
    }
   ],
   "source": [
    "# using str.split() but punctuations such as commas are not stripped/splitted\n",
    "for t in data:\n",
    "    print(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dba5eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('abnormal_LFT', (0, 12)),\n",
       "  ('^^,', (12, 15)),\n",
       "  ('headache', (16, 24)),\n",
       "  ('^^,', (24, 27)),\n",
       "  ('RTI', (28, 31)),\n",
       "  ('^^,', (31, 34)),\n",
       "  ('hemoglobin_decreased', (35, 55)),\n",
       "  ('^^,', (55, 58)),\n",
       "  ('sperm_count_decreased', (59, 80)),\n",
       "  ('^^,', (80, 83)),\n",
       "  ('edema', (84, 89)),\n",
       "  ('^^,', (89, 92)),\n",
       "  ('hepatic_cirrhosis', (93, 110)),\n",
       "  ('(', (110, 111)),\n",
       "  ('pm', (111, 113)),\n",
       "  ('),', (113, 115)),\n",
       "  ('liver_failure', (116, 129)),\n",
       "  ('(', (129, 130)),\n",
       "  ('pm', (130, 132)),\n",
       "  ('),', (132, 134)),\n",
       "  ('jaundice', (135, 143)),\n",
       "  ('(', (143, 144)),\n",
       "  ('pm', (144, 146)),\n",
       "  ('),', (146, 148)),\n",
       "  ('syncope', (149, 156)),\n",
       "  ('^,', (156, 158)),\n",
       "  ('sinusitis', (159, 168)),\n",
       "  ('^,', (168, 170)),\n",
       "  ('nasal_congestion', (171, 187)),\n",
       "  ('^,', (187, 189)),\n",
       "  ('sinus_congestion', (190, 206)),\n",
       "  ('^,', (206, 208)),\n",
       "  ('rhinitis', (209, 217)),\n",
       "  ('^,', (217, 219)),\n",
       "  ('oropharyngeal_pain', (220, 238)),\n",
       "  ('^,', (238, 240)),\n",
       "  ('epistaxis', (241, 250)),\n",
       "  ('^,', (250, 252)),\n",
       "  ('nasopharyngitis', (253, 268)),\n",
       "  ('^,', (268, 270)),\n",
       "  ('idiopathic_pulmonary_fibrosis', (271, 300)),\n",
       "  ('^,', (300, 302)),\n",
       "  ('anemia', (303, 309)),\n",
       "  ('^,', (309, 311)),\n",
       "  ('hematocrit_decreased', (312, 332)),\n",
       "  ('^,', (332, 334)),\n",
       "  ('thrombocytopenia', (335, 351)),\n",
       "  ('(', (351, 352)),\n",
       "  ('pm', (352, 354)),\n",
       "  ('),', (354, 356)),\n",
       "  ('neutropenia', (357, 368)),\n",
       "  ('(', (368, 369)),\n",
       "  ('pm', (369, 371)),\n",
       "  ('),', (371, 373)),\n",
       "  ('leukopenia', (374, 384)),\n",
       "  ('(', (384, 385)),\n",
       "  ('pm', (385, 387)),\n",
       "  ('),', (387, 389)),\n",
       "  ('flushing', (390, 398)),\n",
       "  ('^,', (398, 400)),\n",
       "  ('hypotension', (401, 412)),\n",
       "  ('^,', (412, 414)),\n",
       "  ('palpitation', (415, 426)),\n",
       "  ('^,', (426, 428)),\n",
       "  ('orthostatic_hypotension', (429, 452)),\n",
       "  ('^,', (452, 454)),\n",
       "  ('unstable_angina', (455, 470)),\n",
       "  ('^,', (470, 472)),\n",
       "  ('hot_flush', (473, 482)),\n",
       "  ('^,', (482, 484)),\n",
       "  ('gastroesophageal_reflux_disease', (485, 516)),\n",
       "  ('^,', (516, 518)),\n",
       "  ('diarrhea', (519, 527)),\n",
       "  ('^,', (527, 529)),\n",
       "  ('pruritus', (530, 538)),\n",
       "  ('^,', (538, 540)),\n",
       "  ('erythema', (541, 549)),\n",
       "  ('^,', (549, 551)),\n",
       "  ('angioedema', (552, 562)),\n",
       "  ('(', (562, 563)),\n",
       "  ('pm', (563, 565)),\n",
       "  ('),', (565, 567)),\n",
       "  ('DRESS', (568, 573)),\n",
       "  ('(', (573, 574)),\n",
       "  ('pm', (574, 576)),\n",
       "  ('),', (576, 578)),\n",
       "  ('rash', (579, 583)),\n",
       "  ('(', (583, 584)),\n",
       "  ('pm', (584, 586)),\n",
       "  ('),', (586, 588)),\n",
       "  ('dermatitis', (589, 599)),\n",
       "  ('(', (599, 600)),\n",
       "  ('pm', (600, 602)),\n",
       "  ('),', (602, 604)),\n",
       "  ('arthralgia', (605, 615)),\n",
       "  ('^,', (615, 617)),\n",
       "  ('joint_swelling', (618, 632)),\n",
       "  ('^,', (632, 634)),\n",
       "  ('blurred_vision', (635, 649)),\n",
       "  ('^,', (649, 651)),\n",
       "  ('chest_pain', (652, 662)),\n",
       "  ('^,', (662, 664)),\n",
       "  ('peripheral_edema', (665, 681)),\n",
       "  ('^,', (681, 683)),\n",
       "  ('influenza_like_illness', (684, 706)),\n",
       "  ('^,', (706, 708)),\n",
       "  ('vertigo', (709, 716)),\n",
       "  ('^,', (716, 718)),\n",
       "  ('fever', (719, 724)),\n",
       "  ('^,', (724, 726)),\n",
       "  ('chest_pain', (727, 737)),\n",
       "  ('^,', (737, 739)),\n",
       "  ('hypersensitivity_reaction', (740, 765)),\n",
       "  ('^,', (765, 767)),\n",
       "  ('anaphylaxis', (768, 779)),\n",
       "  ('(', (779, 780)),\n",
       "  ('pm', (780, 782)),\n",
       "  (')', (782, 783))]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pre_tokenizer will split at white spaces and remove punctuations, and set tokens for each word and each punctuation\n",
    "pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "split_data = [pre_tokenizer.pre_tokenize_str(t) for t in data]\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbd1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, token: [UNK]\n",
      "ID: 1, token: [PAD]\n",
      "ID: 2, token: [CLS]\n",
      "ID: 3, token: [SEP]\n",
      "ID: 4, token: [MASK]\n",
      "ID: 5, token: chest_pain^,\n",
      "ID: 6, token: DRESS(pm),\n",
      "ID: 7, token: RTI^^,\n",
      "ID: 8, token: abnormal_LFT^^,\n",
      "ID: 9, token: anaphylaxis(pm)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'ID: {i}, token: {tokenizer.id_to_token(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717cc361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique tokens (words)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d57a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable padding\n",
    "# need to find out if pad_id is always necessary e.g. pad_id = tokenizer.token_to_id(PAD_TOKEN)\n",
    "tokenizer.enable_padding(pad_token=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa19af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 5]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode('vertigo^,', 'chest_pain^,')\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fac569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vertigo^, chest_pain^,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([51, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5713916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be35a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ] at 2025-05-20 14:24:19.085140\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn.functional import one_hot\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import numpy as np\n",
    "# import datamol as dm\n",
    "# import rdkit\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdFingerprintGenerator\n",
    "# import useful_rdkit_utils as uru\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import sys, datetime\n",
    "\n",
    "# print(f\"Pandas version used: {pd.__version__}\")\n",
    "# print(f\"PyTorch version used: {torch.__version__}\")\n",
    "# print(f\"NumPy version used: {np.__version__}\")\n",
    "#print(f\"RDKit version used: {rdkit.__version__}\")\n",
    "print(f\"Python version used: {sys.version} at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70112824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch example re. saving & reloading tensors\n",
    "# t = torch.tensor([1., 2.])\n",
    "# torch.save(t, 'tensor.pt')\n",
    "# ts = torch.load('tensor.pt')\n",
    "# ts\n",
    "\n",
    "\n",
    "# Load adrs tensors from 2_ADR_regressor.ipynb after it's saved (from 2_ADR_regressor_save_tensors.ipynb)\n",
    "# adrs_ts = torch.load(\"adr_train_tensors.pt\")\n",
    "# adrs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bc0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "# note: some of the pre-trained models are freely available but some of them may be gated \n",
    "# (possibly still freely available but may require signing up a HF account)\n",
    "# BERT base transformer model (cased -> case-sensitive) has been used - https://huggingface.co/google-bert/bert-base-cased\n",
    "# \"uncased\" version - https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), \" \\\n",
    "\"liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, \" \\\n",
    "\"epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), \" \\\n",
    "\"leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, \" \\\n",
    "\"gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), \" \\\n",
    "\"arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, \" \\\n",
    "\"chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5beb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db45ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167, 117, 155, 21669, 167, 167, 117, 23123, 8032, 2858, 7939, 168, 10558, 167, 167, 117, 20479, 168, 5099, 168, 10558, 167, 167, 117, 5048, 14494, 167, 167, 117, 1119, 4163, 2941, 168, 172, 3161, 1197, 15342, 1548, 113, 9852, 114, 117, 11911, 168, 4290, 113, 9852, 114, 117, 179, 3984, 12090, 2093, 113, 9852, 114, 117, 188, 27250, 15622, 167, 117, 11850, 1361, 10721, 167, 117, 21447, 168, 22860, 167, 117, 11850, 1361, 168, 22860, 167, 117, 187, 8265, 10721, 167, 117, 1137, 4184, 7111, 5730, 2176, 1348, 168, 2489, 167, 117, 174, 19093, 1777, 8745, 1116, 167, 117, 9468, 7301, 20695, 15023, 24632, 1548, 167, 117, 25021, 2660, 21745, 168, 26600, 168, 20497, 12725, 4863, 167, 117, 1126, 20504, 167, 117, 23123, 10024, 1665, 7729, 168, 10558, 167, 117, 24438, 16071, 4043, 3457, 9870, 23179, 113, 9852, 114, 117, 24928, 3818, 12736, 23179, 113, 9852, 114, 117, 5837, 7563, 15622, 5813, 113, 9852, 114, 117, 14991, 1158, 167, 117, 177, 1183, 11439, 5026, 1988, 167, 117, 185, 1348, 18965, 1891, 167, 117, 1137, 1582, 15540, 7698, 168, 177, 1183, 11439, 5026, 1988, 167, 117, 15443, 168, 1126, 10533, 1161, 167, 117, 2633, 168, 14991, 167, 117, 3245, 8005, 1279, 4184, 19911, 1348, 168, 1231, 2087, 24796, 168, 3653, 167, 117, 4267, 1813, 1197, 13836, 167, 117, 185, 5082, 7729, 1361, 167, 117, 14044, 25669, 15391, 1161, 167, 117, 1126, 10712, 15018, 1918, 113, 9852, 114, 117, 22219, 9919, 1708, 113, 9852, 114, 117, 187, 10733, 113, 9852, 114, 117, 4167, 21943, 10721, 113, 9852, 114, 117, 1893, 20955, 1233, 9037, 167, 117, 4091, 168, 20085, 167, 117, 20611, 168, 4152, 167, 117, 2229, 168, 2489, 167, 117, 17963, 168, 5048, 14494, 167, 117, 1107, 27206, 168, 1176, 168, 6946, 167, 117, 1396, 3740, 11466, 167, 117, 10880, 167, 117, 2229, 168, 2489, 167, 117, 177, 24312, 3792, 5053, 3121, 14499, 168, 3943, 167, 117, 1126, 25890, 22948, 8745, 1116, 113, 9852, 114]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f190530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b4816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abnormal _ LFT ^ ^, headache ^ ^, RTI ^ ^, hemoglobin _ decreased ^ ^, sperm _ count _ decreased ^ ^, edema ^ ^, hepatic _ cirrhosis ( pm ), liver _ failure ( pm ), jaundice ( pm ), syncope ^, sinusitis ^, nasal _ congestion ^, sinus _ congestion ^, rhinitis ^, oropharyngeal _ pain ^, epistaxis ^, nasopharyngitis ^, idiopathic _ pulmonary _ fibrosis ^, anemia ^, hematocrit _ decreased ^, thrombocytopenia ( pm ), neutropenia ( pm ), leukopenia ( pm ), flushing ^, hypotension ^, palpitation ^, orthostatic _ hypotension ^, unstable _ angina ^, hot _ flush ^, gastroesophageal _ reflux _ disease ^, diarrhea ^, pruritus ^, erythema ^, angioedema ( pm ), DRESS ( pm ), rash ( pm ), dermatitis ( pm ), arthralgia ^, joint _ swelling ^, blurred _ vision ^, chest _ pain ^, peripheral _ edema ^, influenza _ like _ illness ^, vertigo ^, fever ^, chest _ pain ^, hypersensitivity _ reaction ^, anaphylaxis ( pm )'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert_tokens_to_string() - merges sub-word tokens into complete words\n",
    "adrs_words = tokenizer.convert_tokens_to_string(tokens)\n",
    "adrs_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36534e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "# convert_ids_to_tokens() - converts numerical IDs back into corresponding token identifiers\n",
    "token_words = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9c689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35738e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnormal _ LFT ^ ^, headache ^ ^\n"
     ]
    }
   ],
   "source": [
    "# example to obtain ADR terms from vocabulary indices\n",
    "adrs_terms = tokenizer.decode([22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167])\n",
    "print(adrs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0615ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try converting the token ID outputs into torch tensors so they can be used in a pytorch model later\n",
    "# transformers models expect multiple lines of string sequences, so likely need to add tensor dimensions and/or paddings later \n",
    "# may be applicable to one line string sequence or multiple string sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e02e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fa1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 22832,   168,   149, 26321,   167,   167,   117, 16320,   167,\n",
       "           167,   117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,\n",
       "          7939,   168, 10558,   167,   167,   117, 20479,   168,  5099,   168,\n",
       "         10558,   167,   167,   117,  5048, 14494,   167,   167,   117,  1119,\n",
       "          4163,  2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,\n",
       "           114,   117, 11911,   168,  4290,   113,  9852,   114,   117,   179,\n",
       "          3984, 12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,\n",
       "           167,   117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,\n",
       "           167,   117, 11850,  1361,   168, 22860,   167,   117,   187,  8265,\n",
       "         10721,   167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,\n",
       "          2489,   167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,\n",
       "          9468,  7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660,\n",
       "         21745,   168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126,\n",
       "         20504,   167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,\n",
       "           117, 24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,\n",
       "           117, 24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,\n",
       "          7563, 15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,\n",
       "           117,   177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348,\n",
       "         18965,  1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,\n",
       "          1183, 11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,\n",
       "          1161,   167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,\n",
       "          1279,  4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,\n",
       "           167,   117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,\n",
       "          7729,  1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,\n",
       "          1126, 10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,\n",
       "          1708,   113,  9852,   114,   117,   187, 10733,   113,  9852,   114,\n",
       "           117,  4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,\n",
       "          1233,  9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,\n",
       "           168,  4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,\n",
       "           168,  5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,\n",
       "          6946,   167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,\n",
       "           117,  2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,\n",
       "          3121, 14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,\n",
       "          1116,   113,  9852,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# API for PreTrainedTokenizerBase class re. parameter on return_tensors \n",
    "# https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__.return_tensors\n",
    "tokenised_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokenised_inputs\n",
    "# output contains \"input_ids\" tensors, \"token_type_ids\" tensors & \"attention_mask\" tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 22832,   168,   149, 26321,   167,   167,   117, 16320,   167,\n",
      "           167,   117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,\n",
      "          7939,   168, 10558,   167,   167,   117, 20479,   168,  5099,   168,\n",
      "         10558,   167,   167,   117,  5048, 14494,   167,   167,   117,  1119,\n",
      "          4163,  2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,\n",
      "           114,   117, 11911,   168,  4290,   113,  9852,   114,   117,   179,\n",
      "          3984, 12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,\n",
      "           167,   117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,\n",
      "           167,   117, 11850,  1361,   168, 22860,   167,   117,   187,  8265,\n",
      "         10721,   167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,\n",
      "          2489,   167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,\n",
      "          9468,  7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660,\n",
      "         21745,   168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126,\n",
      "         20504,   167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,\n",
      "           117, 24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,\n",
      "           117, 24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,\n",
      "          7563, 15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,\n",
      "           117,   177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348,\n",
      "         18965,  1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,\n",
      "          1183, 11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,\n",
      "          1161,   167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,\n",
      "          1279,  4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,\n",
      "           167,   117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,\n",
      "          7729,  1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,\n",
      "          1126, 10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,\n",
      "          1708,   113,  9852,   114,   117,   187, 10733,   113,  9852,   114,\n",
      "           117,  4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,\n",
      "          1233,  9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,\n",
      "           168,  4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,\n",
      "           168,  5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,\n",
      "          6946,   167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,\n",
      "           117,  2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,\n",
      "          3121, 14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,\n",
      "          1116,   113,  9852,   114,   102]])\n"
     ]
    }
   ],
   "source": [
    "# printing out only the \"input_ids\" tensors\n",
    "print(tokenised_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171958a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5565da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22832,   168,   149, 26321,   167,   167,   117, 16320,   167,   167,\n",
       "          117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,  7939,\n",
       "          168, 10558,   167,   167,   117, 20479,   168,  5099,   168, 10558,\n",
       "          167,   167,   117,  5048, 14494,   167,   167,   117,  1119,  4163,\n",
       "         2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,   114,\n",
       "          117, 11911,   168,  4290,   113,  9852,   114,   117,   179,  3984,\n",
       "        12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,   167,\n",
       "          117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,   167,\n",
       "          117, 11850,  1361,   168, 22860,   167,   117,   187,  8265, 10721,\n",
       "          167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,  2489,\n",
       "          167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,  9468,\n",
       "         7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660, 21745,\n",
       "          168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126, 20504,\n",
       "          167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,   117,\n",
       "        24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,   117,\n",
       "        24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,  7563,\n",
       "        15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,   117,\n",
       "          177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348, 18965,\n",
       "         1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,  1183,\n",
       "        11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,  1161,\n",
       "          167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,  1279,\n",
       "         4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,   167,\n",
       "          117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,  7729,\n",
       "         1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,  1126,\n",
       "        10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,  1708,\n",
       "          113,  9852,   114,   117,   187, 10733,   113,  9852,   114,   117,\n",
       "         4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,  1233,\n",
       "         9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,   168,\n",
       "         4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,   168,\n",
       "         5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,  6946,\n",
       "          167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,   117,\n",
       "         2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,  3121,\n",
       "        14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,  1116,\n",
       "          113,  9852,   114])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using pytorch directly to create tensors from token IDs\n",
    "import torch\n",
    "torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sample checkpoint & model with the tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Sample multiple sequence data using ADRs of bosentan and carbamazepine\n",
    "sequence = [\"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), \" \\\n",
    "\"liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, \" \\\n",
    "\"epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), \" \\\n",
    "\"neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, \" \\\n",
    "\"hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), \" \\\n",
    "\"dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, \" \\\n",
    "\"vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\", \"constipation^^, leucopenia^^, dizziness^^, \" \\\n",
    "\"sedation^^, ataxia^^, elevated_GGT^^, allergic_skin_reactions^^, eosinophilia^, thrombocytopenia^, neutropenia^, headache^, \" \\\n",
    "\"tremor^, elevated_ALP^, pruritus^, paresthesia^, diplopia^, blurred_vision^, hyponatremia^, fluid_retention^, oedema^, \"\n",
    "\"weight_gain^, reduced_plasma_osmolarity_(ADH_like_effect)^, vertigo^\"]\n",
    "\n",
    "tokens = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# input_ids = torch.tensor(ids)\n",
    "# print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2043, -2.7013],\n",
       "        [ 2.1903, -1.8928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d6703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19470,\n",
       "  1035,\n",
       "  1048,\n",
       "  6199,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19387,\n",
       "  2072,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  8649,\n",
       "  4135,\n",
       "  8428,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  18047,\n",
       "  1035,\n",
       "  4175,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  2002,\n",
       "  24952,\n",
       "  2278,\n",
       "  1035,\n",
       "  25022,\n",
       "  12171,\n",
       "  25229,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11290,\n",
       "  1035,\n",
       "  4945,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  14855,\n",
       "  8630,\n",
       "  6610,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  26351,\n",
       "  17635,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  13706,\n",
       "  1034,\n",
       "  1010,\n",
       "  19077,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  1054,\n",
       "  20535,\n",
       "  7315,\n",
       "  1034,\n",
       "  1010,\n",
       "  20298,\n",
       "  21890,\n",
       "  18143,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  4958,\n",
       "  11921,\n",
       "  9048,\n",
       "  2015,\n",
       "  1034,\n",
       "  1010,\n",
       "  17235,\n",
       "  7361,\n",
       "  8167,\n",
       "  6038,\n",
       "  23806,\n",
       "  2483,\n",
       "  1034,\n",
       "  1010,\n",
       "  8909,\n",
       "  3695,\n",
       "  25940,\n",
       "  1035,\n",
       "  21908,\n",
       "  1035,\n",
       "  10882,\n",
       "  12618,\n",
       "  6190,\n",
       "  1034,\n",
       "  1010,\n",
       "  2019,\n",
       "  17577,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  10610,\n",
       "  26775,\n",
       "  4183,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  3393,\n",
       "  6968,\n",
       "  26915,\n",
       "  2401,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23519,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14412,\n",
       "  23270,\n",
       "  3370,\n",
       "  1034,\n",
       "  1010,\n",
       "  2030,\n",
       "  2705,\n",
       "  28696,\n",
       "  4588,\n",
       "  1035,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14480,\n",
       "  1035,\n",
       "  17076,\n",
       "  3981,\n",
       "  1034,\n",
       "  1010,\n",
       "  2980,\n",
       "  1035,\n",
       "  13862,\n",
       "  1034,\n",
       "  1010,\n",
       "  3806,\n",
       "  13181,\n",
       "  2229,\n",
       "  7361,\n",
       "  3270,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  25416,\n",
       "  25148,\n",
       "  1035,\n",
       "  4295,\n",
       "  1034,\n",
       "  1010,\n",
       "  22939,\n",
       "  12171,\n",
       "  20192,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  9413,\n",
       "  26688,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  17076,\n",
       "  3695,\n",
       "  14728,\n",
       "  2863,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4377,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23438,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4315,\n",
       "  18900,\n",
       "  13706,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  2396,\n",
       "  13492,\n",
       "  2140,\n",
       "  10440,\n",
       "  1034,\n",
       "  1010,\n",
       "  4101,\n",
       "  1035,\n",
       "  18348,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  15965,\n",
       "  1035,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1010,\n",
       "  24442,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  7355,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034,\n",
       "  1010,\n",
       "  9016,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  23760,\n",
       "  5054,\n",
       "  28032,\n",
       "  7730,\n",
       "  1035,\n",
       "  4668,\n",
       "  1034,\n",
       "  1010,\n",
       "  9617,\n",
       "  21281,\n",
       "  2721,\n",
       "  9048,\n",
       "  2015,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  9530,\n",
       "  16643,\n",
       "  24952,\n",
       "  2239,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3393,\n",
       "  14194,\n",
       "  26915,\n",
       "  2401,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  4487,\n",
       "  29212,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  7367,\n",
       "  20207,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  29533,\n",
       "  14787,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  1043,\n",
       "  13512,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  27395,\n",
       "  1035,\n",
       "  3096,\n",
       "  1035,\n",
       "  9597,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  1041,\n",
       "  20049,\n",
       "  3630,\n",
       "  21850,\n",
       "  6632,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1010,\n",
       "  27734,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  2632,\n",
       "  2361,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  11968,\n",
       "  25344,\n",
       "  1034,\n",
       "  1010,\n",
       "  16510,\n",
       "  4135,\n",
       "  19312,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  7856,\n",
       "  7913,\n",
       "  10092,\n",
       "  1034,\n",
       "  1010,\n",
       "  8331,\n",
       "  1035,\n",
       "  20125,\n",
       "  1034,\n",
       "  1010,\n",
       "  1051,\n",
       "  14728,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  3635,\n",
       "  1035,\n",
       "  5114,\n",
       "  1034,\n",
       "  1010,\n",
       "  4359,\n",
       "  1035,\n",
       "  12123,\n",
       "  1035,\n",
       "  9808,\n",
       "  5302,\n",
       "  8017,\n",
       "  3012,\n",
       "  1035,\n",
       "  1006,\n",
       "  4748,\n",
       "  2232,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  3466,\n",
       "  1007,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034],\n",
       " [19470,\n",
       "  1035,\n",
       "  1048,\n",
       "  6199,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19387,\n",
       "  2072,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  8649,\n",
       "  4135,\n",
       "  8428,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  18047,\n",
       "  1035,\n",
       "  4175,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  2002,\n",
       "  24952,\n",
       "  2278,\n",
       "  1035,\n",
       "  25022,\n",
       "  12171,\n",
       "  25229,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11290,\n",
       "  1035,\n",
       "  4945,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  14855,\n",
       "  8630,\n",
       "  6610,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  26351,\n",
       "  17635,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  13706,\n",
       "  1034,\n",
       "  1010,\n",
       "  19077,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  1054,\n",
       "  20535,\n",
       "  7315,\n",
       "  1034,\n",
       "  1010,\n",
       "  20298,\n",
       "  21890,\n",
       "  18143,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  4958,\n",
       "  11921,\n",
       "  9048,\n",
       "  2015,\n",
       "  1034,\n",
       "  1010,\n",
       "  17235,\n",
       "  7361,\n",
       "  8167,\n",
       "  6038,\n",
       "  23806,\n",
       "  2483,\n",
       "  1034,\n",
       "  1010,\n",
       "  8909,\n",
       "  3695,\n",
       "  25940,\n",
       "  1035,\n",
       "  21908,\n",
       "  1035,\n",
       "  10882,\n",
       "  12618,\n",
       "  6190,\n",
       "  1034,\n",
       "  1010,\n",
       "  2019,\n",
       "  17577,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  10610,\n",
       "  26775,\n",
       "  4183,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  3393,\n",
       "  6968,\n",
       "  26915,\n",
       "  2401,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23519,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14412,\n",
       "  23270,\n",
       "  3370,\n",
       "  1034,\n",
       "  1010,\n",
       "  2030,\n",
       "  2705,\n",
       "  28696,\n",
       "  4588,\n",
       "  1035,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14480,\n",
       "  1035,\n",
       "  17076,\n",
       "  3981,\n",
       "  1034,\n",
       "  1010,\n",
       "  2980,\n",
       "  1035,\n",
       "  13862,\n",
       "  1034,\n",
       "  1010,\n",
       "  3806,\n",
       "  13181,\n",
       "  2229,\n",
       "  7361,\n",
       "  3270,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  25416,\n",
       "  25148,\n",
       "  1035,\n",
       "  4295,\n",
       "  1034,\n",
       "  1010,\n",
       "  22939,\n",
       "  12171,\n",
       "  20192,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  9413,\n",
       "  26688,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  17076,\n",
       "  3695,\n",
       "  14728,\n",
       "  2863,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4377,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23438,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4315,\n",
       "  18900,\n",
       "  13706,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  2396,\n",
       "  13492,\n",
       "  2140,\n",
       "  10440,\n",
       "  1034,\n",
       "  1010,\n",
       "  4101,\n",
       "  1035,\n",
       "  18348,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  15965,\n",
       "  1035,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1010,\n",
       "  24442,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  7355,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034,\n",
       "  1010,\n",
       "  9016,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  23760,\n",
       "  5054,\n",
       "  28032,\n",
       "  7730,\n",
       "  1035,\n",
       "  4668,\n",
       "  1034,\n",
       "  1010,\n",
       "  9617,\n",
       "  21281,\n",
       "  2721,\n",
       "  9048,\n",
       "  2015,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  9530,\n",
       "  16643,\n",
       "  24952,\n",
       "  2239,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3393,\n",
       "  14194,\n",
       "  26915,\n",
       "  2401,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  4487,\n",
       "  29212,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  7367,\n",
       "  20207,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  29533,\n",
       "  14787,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  1043,\n",
       "  13512,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  27395,\n",
       "  1035,\n",
       "  3096,\n",
       "  1035,\n",
       "  9597,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  1041,\n",
       "  20049,\n",
       "  3630,\n",
       "  21850,\n",
       "  6632,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1010,\n",
       "  27734,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  2632,\n",
       "  2361,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  11968,\n",
       "  25344,\n",
       "  1034,\n",
       "  1010,\n",
       "  16510,\n",
       "  4135,\n",
       "  19312,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  7856,\n",
       "  7913,\n",
       "  10092,\n",
       "  1034,\n",
       "  1010,\n",
       "  8331,\n",
       "  1035,\n",
       "  20125,\n",
       "  1034,\n",
       "  1010,\n",
       "  1051,\n",
       "  14728,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  3635,\n",
       "  1035,\n",
       "  5114,\n",
       "  1034,\n",
       "  1010,\n",
       "  4359,\n",
       "  1035,\n",
       "  12123,\n",
       "  1035,\n",
       "  9808,\n",
       "  5302,\n",
       "  8017,\n",
       "  3012,\n",
       "  1035,\n",
       "  1006,\n",
       "  4748,\n",
       "  2232,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  3466,\n",
       "  1007,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making a sample batch of token IDs by using the same sequence twice\n",
    "batched_ids = [ids, ids]\n",
    "batched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1bb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 2.1536, -1.9253],\n",
      "        [ 2.1536, -1.9253]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_batched_ids = torch.tensor(batched_ids)\n",
    "output_batched = model(input_batched_ids)\n",
    "print(\"Logits:\", output_batched.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention masks are used to tell the attention layers (which contextualise each token) in transformer models \n",
    "# to ignore the padding tokens when multiple sequences are of different lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509a509",
   "metadata": {},
   "source": [
    "##### **Some initial thoughts after trying out tokenization**\n",
    "\n",
    "The overall concept that I'm getting at the moment is that a language model (whether large or small) consists of: \n",
    "\n",
    "* training a data corpus\n",
    "* using tokenizer to encode or decode text data\n",
    "* using pre-trained model of choice as a base model to train the data provided\n",
    "* producing training output\n",
    "\n",
    "The pre-trained model can be further adjusted or fine-tuned via training the model on smaller high-quality datasets for other more specific NLP tasks.\n",
    "    \n",
    "This means my initial small goal to convert the tensor outputs back into words will actually be the next step after having the training output from a language translation/summarisation/classification model, meaning I'll have to test the trained model on a different set of test data in order to see if the conversion from tensors to strings will make sense (this leads to the latest new plan to try doing NER for the ADRs of tyrosine kinase inhibitors). \n",
    "\n",
    "* possible training workflow of a small part of an early ADR prediction model may be like this: \n",
    "\n",
    "    input training ADR strings (later may add the \"drug\" part) -> encode into token IDs for training -> tensors -> token IDs to be decoded -> ADR strings\n",
    "\n",
    "    code example for the tensors to token IDs to string representations part: \n",
    "    \n",
    "    tokenizer.batch_decode(outputs.context_input_ids, skip_special_tokens=True)\n",
    "\n",
    "* possible workflow to test data in the trained pre-trained ADR decoder model may be like this:\n",
    "\n",
    "    input testing drug-ADRs -> token IDs -> tensors -> token IDs -> ADR strings\n",
    "\n",
    "\n",
    "A useful and informative reference paper to learn about NLP in drug discovery is by Withers et al. - https://doi.org/10.1080/17460441.2025.2490835"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
