{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85beaa1",
   "metadata": {},
   "source": [
    "##### **About tokenisation (or tokenization)**\n",
    "\n",
    "*draft mode*\n",
    "\n",
    "This is really just a short piece to explore ways to tokenise texts out of my own interests (it may be useful in the future...?). There appears to be two different ways to do this (there may be other methods as well). The first one is a short demonstration about how to use [tokenizers](https://pypi.org/project/tokenizers/) package to tokenise a very small set of adverse drug reaction (ADR) terms or words into tokens, then encode the ADR terms with token IDs, followed by a final decoding of these token IDs back into the corresponding ADR terms. The second one is a short demonstration about using AutoTokenizer from [transformers](https://pypi.org/project/transformers/) package to tokenise or encode and then decode one line of texts. \n",
    "\n",
    "Reference links: \n",
    "* https://huggingface.co/learn/llm-course/chapter2/4?fw=pt#tokenization\n",
    "* https://huggingface.co/docs/datasets/use_dataset#tokenize-text\n",
    "\n",
    "My original plan looks like this:\n",
    "* initial small goal is to try using tokenizer.decode(), so building a tokenization or tokenizer model first\n",
    "* the idea is that this text data tokenisation part may be added to a larger DNN model to decode ADRs output later (subject to further idea changes... may try a small NER classification model first, see 6_NER_tk_inhibitors.ipynb)\n",
    "\n",
    "* trying HuggingFace's transformers package with possible steps like this:\n",
    "1. set up tokenizer model that will tokenize the ADRs/words\n",
    "2. apply tokenizer.decode() function to each tensor row/sequence (via using list comprehension)\n",
    "3. use sample code snippet below to decode tensors: \n",
    "e.g. decoded = [tokenizer.decode(x) for x in adrs_ts]\n",
    "- the code will iterate through each row/sequence of tensors and apply the decode() method which'll transform the numerical IDs back into human-readable texts/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd095a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ] at 2025-06-19 19:40:17.637191\n"
     ]
    }
   ],
   "source": [
    "#from tokenizers.models import WordLevel\n",
    "import torch\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import sys, datetime\n",
    "print(f\"Python version used: {sys.version} at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70a95f",
   "metadata": {},
   "source": [
    "##### **Using tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7138af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample normalizers code to \"normalise\" texts\n",
    "# somehow the normalizer code is not quite working yet... text data in and the same text data out...\n",
    "\n",
    "# from tokenizers.models import BPE, WordLevel, WordPiece\n",
    "# from tokenizers import Tokenizer, normalizers\n",
    "# from tokenizers.normalizers import StripAccents, Sequence, Replace\n",
    "\n",
    "# BPE - byte pair encoding\n",
    "# bpe_tokenizer = Tokenizer(BPE())\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "# bpe_tokenizer.normalizer = normalizers.Sequence([StripAccents()])\n",
    "## normalizer seems to be set already even though code seems not right within the normalizers.Sequence() (?)\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "\n",
    "# sentences = ['abdominal_pain', 'Höw aRę ŸõŪ dÔįñg?']\n",
    "\n",
    "# normalized_sentences = [bpe_tokenizer.normalizer.normalize_str(s) for s in sentences]\n",
    "# normalized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ac0c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gastroesophageal_reflux_disease^,': 21,\n",
       " 'hematocrit_decreased^,': 23,\n",
       " 'epistaxis^,': 17,\n",
       " 'hypotension^,': 28,\n",
       " 'palpitation^,': 40,\n",
       " '[PAD]': 1,\n",
       " 'abnormal_LFT^^,': 8,\n",
       " 'angioedema(pm),': 11,\n",
       " 'neutropenia(pm),': 37,\n",
       " 'RTI^^,': 7,\n",
       " 'hypersensitivity_reaction^,': 27,\n",
       " 'rhinitis^,': 44,\n",
       " 'edema^^,': 16,\n",
       " 'chest_pain^,': 5,\n",
       " 'sinus_congestion^,': 45,\n",
       " 'oropharyngeal_pain^,': 38,\n",
       " '[SEP]': 3,\n",
       " 'DRESS(pm),': 6,\n",
       " '[UNK]': 0,\n",
       " 'sinusitis^,': 46,\n",
       " 'thrombocytopenia(pm),': 49,\n",
       " 'peripheral_edema^,': 41,\n",
       " 'unstable_angina^,': 50,\n",
       " 'diarrhea^,': 15,\n",
       " 'nasopharyngitis^,': 36,\n",
       " '[MASK]': 4,\n",
       " 'idiopathic_pulmonary_fibrosis^,': 29,\n",
       " 'leukopenia(pm),': 33,\n",
       " 'nasal_congestion^,': 35,\n",
       " 'headache^^,': 22,\n",
       " 'sperm_count_decreased^^,': 47,\n",
       " 'hot_flush^,': 26,\n",
       " 'hepatic_cirrhosis(pm),': 25,\n",
       " 'jaundice(pm),': 31,\n",
       " 'pruritus^,': 42,\n",
       " 'orthostatic_hypotension^,': 39,\n",
       " 'dermatitis(pm),': 14,\n",
       " 'fever^,': 19,\n",
       " 'hemoglobin_decreased^^,': 24,\n",
       " 'liver_failure(pm),': 34,\n",
       " 'rash(pm),': 43,\n",
       " 'syncope^,': 48,\n",
       " 'erythema^,': 18,\n",
       " 'arthralgia^,': 12,\n",
       " 'flushing^,': 20,\n",
       " 'influenza_like_illness^,': 30,\n",
       " 'blurred_vision^,': 13,\n",
       " 'vertigo^,': 51,\n",
       " '[CLS]': 2,\n",
       " 'anemia^,': 10,\n",
       " 'joint_swelling^,': 32,\n",
       " 'anaphylaxis(pm)': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text data from one of CYP3A4 substrates - bosenten's ADRs \n",
    "# since ADRs data are preprocessed a bit more than raw texts found elsewhere, decided to go straight to create a tokenizer\n",
    "data = [\"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"]\n",
    "\n",
    "#UNK_TOKEN = '[UNK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "\n",
    "# have not yet taken into account of unknown words or padding token\n",
    "tokenizer = Tokenizer(models.WordLevel())\n",
    "\n",
    "# below link explains about how to add special tokens e.g. unknown tokens to take into account diff. scenarios\n",
    "# https://huggingface.co/learn/llm-course/chapter6/8?fw=pt#building-a-wordpiece-tokenizer-from-scratch\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=100000, special_tokens=special_tokens)\n",
    "\n",
    "# training tokenizer \n",
    "# specify iterator - pass through iterator a sequence of sequences in the data via using map() function to apply split()\n",
    "# and trainer\n",
    "tokenizer.train_from_iterator(map(lambda x: x.split(), data), trainer=trainer)\n",
    "\n",
    "tokenizer.get_vocab()\n",
    "# returns the indices of each token in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c664ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal_LFT^^,', 'headache^^,', 'RTI^^,', 'hemoglobin_decreased^^,', 'sperm_count_decreased^^,', 'edema^^,', 'hepatic_cirrhosis(pm),', 'liver_failure(pm),', 'jaundice(pm),', 'syncope^,', 'sinusitis^,', 'nasal_congestion^,', 'sinus_congestion^,', 'rhinitis^,', 'oropharyngeal_pain^,', 'epistaxis^,', 'nasopharyngitis^,', 'idiopathic_pulmonary_fibrosis^,', 'anemia^,', 'hematocrit_decreased^,', 'thrombocytopenia(pm),', 'neutropenia(pm),', 'leukopenia(pm),', 'flushing^,', 'hypotension^,', 'palpitation^,', 'orthostatic_hypotension^,', 'unstable_angina^,', 'hot_flush^,', 'gastroesophageal_reflux_disease^,', 'diarrhea^,', 'pruritus^,', 'erythema^,', 'angioedema(pm),', 'DRESS(pm),', 'rash(pm),', 'dermatitis(pm),', 'arthralgia^,', 'joint_swelling^,', 'blurred_vision^,', 'chest_pain^,', 'peripheral_edema^,', 'influenza_like_illness^,', 'vertigo^,', 'fever^,', 'chest_pain^,', 'hypersensitivity_reaction^,', 'anaphylaxis(pm)']\n"
     ]
    }
   ],
   "source": [
    "# using str.split() but punctuations such as commas are not stripped/splitted\n",
    "for t in data:\n",
    "    print(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dba5eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('abnormal_LFT', (0, 12)),\n",
       "  ('^^,', (12, 15)),\n",
       "  ('headache', (16, 24)),\n",
       "  ('^^,', (24, 27)),\n",
       "  ('RTI', (28, 31)),\n",
       "  ('^^,', (31, 34)),\n",
       "  ('hemoglobin_decreased', (35, 55)),\n",
       "  ('^^,', (55, 58)),\n",
       "  ('sperm_count_decreased', (59, 80)),\n",
       "  ('^^,', (80, 83)),\n",
       "  ('edema', (84, 89)),\n",
       "  ('^^,', (89, 92)),\n",
       "  ('hepatic_cirrhosis', (93, 110)),\n",
       "  ('(', (110, 111)),\n",
       "  ('pm', (111, 113)),\n",
       "  ('),', (113, 115)),\n",
       "  ('liver_failure', (116, 129)),\n",
       "  ('(', (129, 130)),\n",
       "  ('pm', (130, 132)),\n",
       "  ('),', (132, 134)),\n",
       "  ('jaundice', (135, 143)),\n",
       "  ('(', (143, 144)),\n",
       "  ('pm', (144, 146)),\n",
       "  ('),', (146, 148)),\n",
       "  ('syncope', (149, 156)),\n",
       "  ('^,', (156, 158)),\n",
       "  ('sinusitis', (159, 168)),\n",
       "  ('^,', (168, 170)),\n",
       "  ('nasal_congestion', (171, 187)),\n",
       "  ('^,', (187, 189)),\n",
       "  ('sinus_congestion', (190, 206)),\n",
       "  ('^,', (206, 208)),\n",
       "  ('rhinitis', (209, 217)),\n",
       "  ('^,', (217, 219)),\n",
       "  ('oropharyngeal_pain', (220, 238)),\n",
       "  ('^,', (238, 240)),\n",
       "  ('epistaxis', (241, 250)),\n",
       "  ('^,', (250, 252)),\n",
       "  ('nasopharyngitis', (253, 268)),\n",
       "  ('^,', (268, 270)),\n",
       "  ('idiopathic_pulmonary_fibrosis', (271, 300)),\n",
       "  ('^,', (300, 302)),\n",
       "  ('anemia', (303, 309)),\n",
       "  ('^,', (309, 311)),\n",
       "  ('hematocrit_decreased', (312, 332)),\n",
       "  ('^,', (332, 334)),\n",
       "  ('thrombocytopenia', (335, 351)),\n",
       "  ('(', (351, 352)),\n",
       "  ('pm', (352, 354)),\n",
       "  ('),', (354, 356)),\n",
       "  ('neutropenia', (357, 368)),\n",
       "  ('(', (368, 369)),\n",
       "  ('pm', (369, 371)),\n",
       "  ('),', (371, 373)),\n",
       "  ('leukopenia', (374, 384)),\n",
       "  ('(', (384, 385)),\n",
       "  ('pm', (385, 387)),\n",
       "  ('),', (387, 389)),\n",
       "  ('flushing', (390, 398)),\n",
       "  ('^,', (398, 400)),\n",
       "  ('hypotension', (401, 412)),\n",
       "  ('^,', (412, 414)),\n",
       "  ('palpitation', (415, 426)),\n",
       "  ('^,', (426, 428)),\n",
       "  ('orthostatic_hypotension', (429, 452)),\n",
       "  ('^,', (452, 454)),\n",
       "  ('unstable_angina', (455, 470)),\n",
       "  ('^,', (470, 472)),\n",
       "  ('hot_flush', (473, 482)),\n",
       "  ('^,', (482, 484)),\n",
       "  ('gastroesophageal_reflux_disease', (485, 516)),\n",
       "  ('^,', (516, 518)),\n",
       "  ('diarrhea', (519, 527)),\n",
       "  ('^,', (527, 529)),\n",
       "  ('pruritus', (530, 538)),\n",
       "  ('^,', (538, 540)),\n",
       "  ('erythema', (541, 549)),\n",
       "  ('^,', (549, 551)),\n",
       "  ('angioedema', (552, 562)),\n",
       "  ('(', (562, 563)),\n",
       "  ('pm', (563, 565)),\n",
       "  ('),', (565, 567)),\n",
       "  ('DRESS', (568, 573)),\n",
       "  ('(', (573, 574)),\n",
       "  ('pm', (574, 576)),\n",
       "  ('),', (576, 578)),\n",
       "  ('rash', (579, 583)),\n",
       "  ('(', (583, 584)),\n",
       "  ('pm', (584, 586)),\n",
       "  ('),', (586, 588)),\n",
       "  ('dermatitis', (589, 599)),\n",
       "  ('(', (599, 600)),\n",
       "  ('pm', (600, 602)),\n",
       "  ('),', (602, 604)),\n",
       "  ('arthralgia', (605, 615)),\n",
       "  ('^,', (615, 617)),\n",
       "  ('joint_swelling', (618, 632)),\n",
       "  ('^,', (632, 634)),\n",
       "  ('blurred_vision', (635, 649)),\n",
       "  ('^,', (649, 651)),\n",
       "  ('chest_pain', (652, 662)),\n",
       "  ('^,', (662, 664)),\n",
       "  ('peripheral_edema', (665, 681)),\n",
       "  ('^,', (681, 683)),\n",
       "  ('influenza_like_illness', (684, 706)),\n",
       "  ('^,', (706, 708)),\n",
       "  ('vertigo', (709, 716)),\n",
       "  ('^,', (716, 718)),\n",
       "  ('fever', (719, 724)),\n",
       "  ('^,', (724, 726)),\n",
       "  ('chest_pain', (727, 737)),\n",
       "  ('^,', (737, 739)),\n",
       "  ('hypersensitivity_reaction', (740, 765)),\n",
       "  ('^,', (765, 767)),\n",
       "  ('anaphylaxis', (768, 779)),\n",
       "  ('(', (779, 780)),\n",
       "  ('pm', (780, 782)),\n",
       "  (')', (782, 783))]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pre_tokenizer will split at white spaces and remove punctuations, and set tokens for each word and each punctuation\n",
    "pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "split_data = [pre_tokenizer.pre_tokenize_str(t) for t in data]\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbd1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, token: [UNK]\n",
      "ID: 1, token: [PAD]\n",
      "ID: 2, token: [CLS]\n",
      "ID: 3, token: [SEP]\n",
      "ID: 4, token: [MASK]\n",
      "ID: 5, token: chest_pain^,\n",
      "ID: 6, token: DRESS(pm),\n",
      "ID: 7, token: RTI^^,\n",
      "ID: 8, token: abnormal_LFT^^,\n",
      "ID: 9, token: anaphylaxis(pm)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'ID: {i}, token: {tokenizer.id_to_token(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717cc361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique tokens (words)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d57a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable padding\n",
    "# need to find out if pad_id is always necessary e.g. pad_id = tokenizer.token_to_id(PAD_TOKEN)\n",
    "tokenizer.enable_padding(pad_token=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa19af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 5]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode('vertigo^,', 'chest_pain^,')\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fac569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vertigo^, chest_pain^,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([51, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb068e",
   "metadata": {},
   "source": [
    "##### **Using Autotokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70112824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch example re. saving & reloading tensors\n",
    "# t = torch.tensor([1., 2.])\n",
    "# torch.save(t, 'tensor.pt')\n",
    "# ts = torch.load('tensor.pt')\n",
    "# ts\n",
    "\n",
    "\n",
    "# Load adrs tensors from 2_ADR_regressor.ipynb after it's saved (from 2_ADR_regressor_save_tensors.ipynb)\n",
    "# adrs_ts = torch.load(\"adr_train_tensors.pt\")\n",
    "# adrs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55bc0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "# note: some of the pre-trained models are freely available but some of them may be gated \n",
    "# (possibly still freely available but may require signing up a HF account)\n",
    "# BERT base transformer model (cased -> case-sensitive) has been used - https://huggingface.co/google-bert/bert-base-cased\n",
    "# \"uncased\" version - https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), \" \\\n",
    "\"liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, \" \\\n",
    "\"epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), \" \\\n",
    "\"leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, \" \\\n",
    "\"gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), \" \\\n",
    "\"arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, \" \\\n",
    "\"chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89db45ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167, 117, 155, 21669, 167, 167, 117, 23123, 8032, 2858, 7939, 168, 10558, 167, 167, 117, 20479, 168, 5099, 168, 10558, 167, 167, 117, 5048, 14494, 167, 167, 117, 1119, 4163, 2941, 168, 172, 3161, 1197, 15342, 1548, 113, 9852, 114, 117, 11911, 168, 4290, 113, 9852, 114, 117, 179, 3984, 12090, 2093, 113, 9852, 114, 117, 188, 27250, 15622, 167, 117, 11850, 1361, 10721, 167, 117, 21447, 168, 22860, 167, 117, 11850, 1361, 168, 22860, 167, 117, 187, 8265, 10721, 167, 117, 1137, 4184, 7111, 5730, 2176, 1348, 168, 2489, 167, 117, 174, 19093, 1777, 8745, 1116, 167, 117, 9468, 7301, 20695, 15023, 24632, 1548, 167, 117, 25021, 2660, 21745, 168, 26600, 168, 20497, 12725, 4863, 167, 117, 1126, 20504, 167, 117, 23123, 10024, 1665, 7729, 168, 10558, 167, 117, 24438, 16071, 4043, 3457, 9870, 23179, 113, 9852, 114, 117, 24928, 3818, 12736, 23179, 113, 9852, 114, 117, 5837, 7563, 15622, 5813, 113, 9852, 114, 117, 14991, 1158, 167, 117, 177, 1183, 11439, 5026, 1988, 167, 117, 185, 1348, 18965, 1891, 167, 117, 1137, 1582, 15540, 7698, 168, 177, 1183, 11439, 5026, 1988, 167, 117, 15443, 168, 1126, 10533, 1161, 167, 117, 2633, 168, 14991, 167, 117, 3245, 8005, 1279, 4184, 19911, 1348, 168, 1231, 2087, 24796, 168, 3653, 167, 117, 4267, 1813, 1197, 13836, 167, 117, 185, 5082, 7729, 1361, 167, 117, 14044, 25669, 15391, 1161, 167, 117, 1126, 10712, 15018, 1918, 113, 9852, 114, 117, 22219, 9919, 1708, 113, 9852, 114, 117, 187, 10733, 113, 9852, 114, 117, 4167, 21943, 10721, 113, 9852, 114, 117, 1893, 20955, 1233, 9037, 167, 117, 4091, 168, 20085, 167, 117, 20611, 168, 4152, 167, 117, 2229, 168, 2489, 167, 117, 17963, 168, 5048, 14494, 167, 117, 1107, 27206, 168, 1176, 168, 6946, 167, 117, 1396, 3740, 11466, 167, 117, 10880, 167, 117, 2229, 168, 2489, 167, 117, 177, 24312, 3792, 5053, 3121, 14499, 168, 3943, 167, 117, 1126, 25890, 22948, 8745, 1116, 113, 9852, 114]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "314b4816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abnormal _ LFT ^ ^, headache ^ ^, RTI ^ ^, hemoglobin _ decreased ^ ^, sperm _ count _ decreased ^ ^, edema ^ ^, hepatic _ cirrhosis ( pm ), liver _ failure ( pm ), jaundice ( pm ), syncope ^, sinusitis ^, nasal _ congestion ^, sinus _ congestion ^, rhinitis ^, oropharyngeal _ pain ^, epistaxis ^, nasopharyngitis ^, idiopathic _ pulmonary _ fibrosis ^, anemia ^, hematocrit _ decreased ^, thrombocytopenia ( pm ), neutropenia ( pm ), leukopenia ( pm ), flushing ^, hypotension ^, palpitation ^, orthostatic _ hypotension ^, unstable _ angina ^, hot _ flush ^, gastroesophageal _ reflux _ disease ^, diarrhea ^, pruritus ^, erythema ^, angioedema ( pm ), DRESS ( pm ), rash ( pm ), dermatitis ( pm ), arthralgia ^, joint _ swelling ^, blurred _ vision ^, chest _ pain ^, peripheral _ edema ^, influenza _ like _ illness ^, vertigo ^, fever ^, chest _ pain ^, hypersensitivity _ reaction ^, anaphylaxis ( pm )'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert_tokens_to_string() - merges sub-word tokens into complete words\n",
    "adrs_words = tokenizer.convert_tokens_to_string(tokens)\n",
    "adrs_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36534e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "# convert_ids_to_tokens() - converts numerical IDs back into corresponding token identifiers\n",
    "token_words = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35738e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnormal _ LFT ^ ^, headache ^ ^\n"
     ]
    }
   ],
   "source": [
    "# example to obtain ADR terms from vocabulary indices\n",
    "adrs_terms = tokenizer.decode([22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167])\n",
    "print(adrs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba96c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try converting the token ID outputs into torch tensors so they can be used in a pytorch model later\n",
    "# transformers models expect multiple lines of string sequences, so likely need to add tensor dimensions and/or paddings later \n",
    "# may be applicable to one line string sequence or multiple string sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a5fa1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 22832,   168,   149, 26321,   167,   167,   117, 16320,   167,\n",
       "           167,   117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,\n",
       "          7939,   168, 10558,   167,   167,   117, 20479,   168,  5099,   168,\n",
       "         10558,   167,   167,   117,  5048, 14494,   167,   167,   117,  1119,\n",
       "          4163,  2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,\n",
       "           114,   117, 11911,   168,  4290,   113,  9852,   114,   117,   179,\n",
       "          3984, 12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,\n",
       "           167,   117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,\n",
       "           167,   117, 11850,  1361,   168, 22860,   167,   117,   187,  8265,\n",
       "         10721,   167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,\n",
       "          2489,   167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,\n",
       "          9468,  7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660,\n",
       "         21745,   168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126,\n",
       "         20504,   167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,\n",
       "           117, 24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,\n",
       "           117, 24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,\n",
       "          7563, 15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,\n",
       "           117,   177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348,\n",
       "         18965,  1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,\n",
       "          1183, 11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,\n",
       "          1161,   167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,\n",
       "          1279,  4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,\n",
       "           167,   117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,\n",
       "          7729,  1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,\n",
       "          1126, 10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,\n",
       "          1708,   113,  9852,   114,   117,   187, 10733,   113,  9852,   114,\n",
       "           117,  4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,\n",
       "          1233,  9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,\n",
       "           168,  4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,\n",
       "           168,  5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,\n",
       "          6946,   167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,\n",
       "           117,  2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,\n",
       "          3121, 14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,\n",
       "          1116,   113,  9852,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API for PreTrainedTokenizerBase class re. parameter on return_tensors \n",
    "# https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__.return_tensors\n",
    "tokenised_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokenised_inputs\n",
    "# output contains \"input_ids\" tensors, \"token_type_ids\" tensors & \"attention_mask\" tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b73b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 22832,   168,   149, 26321,   167,   167,   117, 16320,   167,\n",
      "           167,   117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,\n",
      "          7939,   168, 10558,   167,   167,   117, 20479,   168,  5099,   168,\n",
      "         10558,   167,   167,   117,  5048, 14494,   167,   167,   117,  1119,\n",
      "          4163,  2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,\n",
      "           114,   117, 11911,   168,  4290,   113,  9852,   114,   117,   179,\n",
      "          3984, 12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,\n",
      "           167,   117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,\n",
      "           167,   117, 11850,  1361,   168, 22860,   167,   117,   187,  8265,\n",
      "         10721,   167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,\n",
      "          2489,   167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,\n",
      "          9468,  7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660,\n",
      "         21745,   168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126,\n",
      "         20504,   167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,\n",
      "           117, 24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,\n",
      "           117, 24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,\n",
      "          7563, 15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,\n",
      "           117,   177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348,\n",
      "         18965,  1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,\n",
      "          1183, 11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,\n",
      "          1161,   167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,\n",
      "          1279,  4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,\n",
      "           167,   117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,\n",
      "          7729,  1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,\n",
      "          1126, 10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,\n",
      "          1708,   113,  9852,   114,   117,   187, 10733,   113,  9852,   114,\n",
      "           117,  4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,\n",
      "          1233,  9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,\n",
      "           168,  4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,\n",
      "           168,  5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,\n",
      "          6946,   167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,\n",
      "           117,  2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,\n",
      "          3121, 14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,\n",
      "          1116,   113,  9852,   114,   102]])\n"
     ]
    }
   ],
   "source": [
    "# printing out only the \"input_ids\" tensors\n",
    "print(tokenised_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d5565da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22832,   168,   149, 26321,   167,   167,   117, 16320,   167,   167,\n",
       "          117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,  7939,\n",
       "          168, 10558,   167,   167,   117, 20479,   168,  5099,   168, 10558,\n",
       "          167,   167,   117,  5048, 14494,   167,   167,   117,  1119,  4163,\n",
       "         2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,   114,\n",
       "          117, 11911,   168,  4290,   113,  9852,   114,   117,   179,  3984,\n",
       "        12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,   167,\n",
       "          117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,   167,\n",
       "          117, 11850,  1361,   168, 22860,   167,   117,   187,  8265, 10721,\n",
       "          167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,  2489,\n",
       "          167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,  9468,\n",
       "         7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660, 21745,\n",
       "          168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126, 20504,\n",
       "          167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,   117,\n",
       "        24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,   117,\n",
       "        24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,  7563,\n",
       "        15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,   117,\n",
       "          177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348, 18965,\n",
       "         1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,  1183,\n",
       "        11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,  1161,\n",
       "          167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,  1279,\n",
       "         4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,   167,\n",
       "          117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,  7729,\n",
       "         1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,  1126,\n",
       "        10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,  1708,\n",
       "          113,  9852,   114,   117,   187, 10733,   113,  9852,   114,   117,\n",
       "         4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,  1233,\n",
       "         9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,   168,\n",
       "         4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,   168,\n",
       "         5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,  6946,\n",
       "          167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,   117,\n",
       "         2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,  3121,\n",
       "        14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,  1116,\n",
       "          113,  9852,   114])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pytorch directly to create tensors from token IDs\n",
    "import torch\n",
    "torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ff20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sample checkpoint & model with the tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Sample multiple sequence data using ADRs of bosentan and carbamazepine\n",
    "sequence = [\"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), \" \\\n",
    "\"liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, \" \\\n",
    "\"epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), \" \\\n",
    "\"neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, \" \\\n",
    "\"hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), \" \\\n",
    "\"dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, \" \\\n",
    "\"vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\", \"constipation^^, leucopenia^^, dizziness^^, \" \\\n",
    "\"sedation^^, ataxia^^, elevated_GGT^^, allergic_skin_reactions^^, eosinophilia^, thrombocytopenia^, neutropenia^, headache^, \" \\\n",
    "\"tremor^, elevated_ALP^, pruritus^, paresthesia^, diplopia^, blurred_vision^, hyponatremia^, fluid_retention^, oedema^, \"\n",
    "\"weight_gain^, reduced_plasma_osmolarity_(ADH_like_effect)^, vertigo^\"]\n",
    "\n",
    "tokens = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# input_ids = torch.tensor(ids)\n",
    "# print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ca0840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2043, -2.7013],\n",
       "        [ 2.1903, -1.8928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b64d6703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22832,\n",
       "  168,\n",
       "  149,\n",
       "  26321,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  16320,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  155,\n",
       "  21669,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  23123,\n",
       "  8032,\n",
       "  2858,\n",
       "  7939,\n",
       "  168,\n",
       "  10558,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  20479,\n",
       "  168,\n",
       "  5099,\n",
       "  168,\n",
       "  10558,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  5048,\n",
       "  14494,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  1119,\n",
       "  4163,\n",
       "  2941,\n",
       "  168,\n",
       "  172,\n",
       "  3161,\n",
       "  1197,\n",
       "  15342,\n",
       "  1548,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  11911,\n",
       "  168,\n",
       "  4290,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  179,\n",
       "  3984,\n",
       "  12090,\n",
       "  2093,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  188,\n",
       "  27250,\n",
       "  15622,\n",
       "  167,\n",
       "  117,\n",
       "  11850,\n",
       "  1361,\n",
       "  10721,\n",
       "  167,\n",
       "  117,\n",
       "  21447,\n",
       "  168,\n",
       "  22860,\n",
       "  167,\n",
       "  117,\n",
       "  11850,\n",
       "  1361,\n",
       "  168,\n",
       "  22860,\n",
       "  167,\n",
       "  117,\n",
       "  187,\n",
       "  8265,\n",
       "  10721,\n",
       "  167,\n",
       "  117,\n",
       "  1137,\n",
       "  4184,\n",
       "  7111,\n",
       "  5730,\n",
       "  2176,\n",
       "  1348,\n",
       "  168,\n",
       "  2489,\n",
       "  167,\n",
       "  117,\n",
       "  174,\n",
       "  19093,\n",
       "  1777,\n",
       "  8745,\n",
       "  1116,\n",
       "  167,\n",
       "  117,\n",
       "  9468,\n",
       "  7301,\n",
       "  20695,\n",
       "  15023,\n",
       "  24632,\n",
       "  1548,\n",
       "  167,\n",
       "  117,\n",
       "  25021,\n",
       "  2660,\n",
       "  21745,\n",
       "  168,\n",
       "  26600,\n",
       "  168,\n",
       "  20497,\n",
       "  12725,\n",
       "  4863,\n",
       "  167,\n",
       "  117,\n",
       "  1126,\n",
       "  20504,\n",
       "  167,\n",
       "  117,\n",
       "  23123,\n",
       "  10024,\n",
       "  1665,\n",
       "  7729,\n",
       "  168,\n",
       "  10558,\n",
       "  167,\n",
       "  117,\n",
       "  24438,\n",
       "  16071,\n",
       "  4043,\n",
       "  3457,\n",
       "  9870,\n",
       "  23179,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  24928,\n",
       "  3818,\n",
       "  12736,\n",
       "  23179,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  5837,\n",
       "  7563,\n",
       "  15622,\n",
       "  5813,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  14991,\n",
       "  1158,\n",
       "  167,\n",
       "  117,\n",
       "  177,\n",
       "  1183,\n",
       "  11439,\n",
       "  5026,\n",
       "  1988,\n",
       "  167,\n",
       "  117,\n",
       "  185,\n",
       "  1348,\n",
       "  18965,\n",
       "  1891,\n",
       "  167,\n",
       "  117,\n",
       "  1137,\n",
       "  1582,\n",
       "  15540,\n",
       "  7698,\n",
       "  168,\n",
       "  177,\n",
       "  1183,\n",
       "  11439,\n",
       "  5026,\n",
       "  1988,\n",
       "  167,\n",
       "  117,\n",
       "  15443,\n",
       "  168,\n",
       "  1126,\n",
       "  10533,\n",
       "  1161,\n",
       "  167,\n",
       "  117,\n",
       "  2633,\n",
       "  168,\n",
       "  14991,\n",
       "  167,\n",
       "  117,\n",
       "  3245,\n",
       "  8005,\n",
       "  1279,\n",
       "  4184,\n",
       "  19911,\n",
       "  1348,\n",
       "  168,\n",
       "  1231,\n",
       "  2087,\n",
       "  24796,\n",
       "  168,\n",
       "  3653,\n",
       "  167,\n",
       "  117,\n",
       "  4267,\n",
       "  1813,\n",
       "  1197,\n",
       "  13836,\n",
       "  167,\n",
       "  117,\n",
       "  185,\n",
       "  5082,\n",
       "  7729,\n",
       "  1361,\n",
       "  167,\n",
       "  117,\n",
       "  14044,\n",
       "  25669,\n",
       "  15391,\n",
       "  1161,\n",
       "  167,\n",
       "  117,\n",
       "  1126,\n",
       "  10712,\n",
       "  15018,\n",
       "  1918,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  22219,\n",
       "  9919,\n",
       "  1708,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  187,\n",
       "  10733,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  4167,\n",
       "  21943,\n",
       "  10721,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  1893,\n",
       "  20955,\n",
       "  1233,\n",
       "  9037,\n",
       "  167,\n",
       "  117,\n",
       "  4091,\n",
       "  168,\n",
       "  20085,\n",
       "  167,\n",
       "  117,\n",
       "  20611,\n",
       "  168,\n",
       "  4152,\n",
       "  167,\n",
       "  117,\n",
       "  2229,\n",
       "  168,\n",
       "  2489,\n",
       "  167,\n",
       "  117,\n",
       "  17963,\n",
       "  168,\n",
       "  5048,\n",
       "  14494,\n",
       "  167,\n",
       "  117,\n",
       "  1107,\n",
       "  27206,\n",
       "  168,\n",
       "  1176,\n",
       "  168,\n",
       "  6946,\n",
       "  167,\n",
       "  117,\n",
       "  1396,\n",
       "  3740,\n",
       "  11466,\n",
       "  167,\n",
       "  117,\n",
       "  10880,\n",
       "  167,\n",
       "  117,\n",
       "  2229,\n",
       "  168,\n",
       "  2489,\n",
       "  167,\n",
       "  117,\n",
       "  177,\n",
       "  24312,\n",
       "  3792,\n",
       "  5053,\n",
       "  3121,\n",
       "  14499,\n",
       "  168,\n",
       "  3943,\n",
       "  167,\n",
       "  117,\n",
       "  1126,\n",
       "  25890,\n",
       "  22948,\n",
       "  8745,\n",
       "  1116,\n",
       "  113,\n",
       "  9852,\n",
       "  114],\n",
       " [22832,\n",
       "  168,\n",
       "  149,\n",
       "  26321,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  16320,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  155,\n",
       "  21669,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  23123,\n",
       "  8032,\n",
       "  2858,\n",
       "  7939,\n",
       "  168,\n",
       "  10558,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  20479,\n",
       "  168,\n",
       "  5099,\n",
       "  168,\n",
       "  10558,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  5048,\n",
       "  14494,\n",
       "  167,\n",
       "  167,\n",
       "  117,\n",
       "  1119,\n",
       "  4163,\n",
       "  2941,\n",
       "  168,\n",
       "  172,\n",
       "  3161,\n",
       "  1197,\n",
       "  15342,\n",
       "  1548,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  11911,\n",
       "  168,\n",
       "  4290,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  179,\n",
       "  3984,\n",
       "  12090,\n",
       "  2093,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  188,\n",
       "  27250,\n",
       "  15622,\n",
       "  167,\n",
       "  117,\n",
       "  11850,\n",
       "  1361,\n",
       "  10721,\n",
       "  167,\n",
       "  117,\n",
       "  21447,\n",
       "  168,\n",
       "  22860,\n",
       "  167,\n",
       "  117,\n",
       "  11850,\n",
       "  1361,\n",
       "  168,\n",
       "  22860,\n",
       "  167,\n",
       "  117,\n",
       "  187,\n",
       "  8265,\n",
       "  10721,\n",
       "  167,\n",
       "  117,\n",
       "  1137,\n",
       "  4184,\n",
       "  7111,\n",
       "  5730,\n",
       "  2176,\n",
       "  1348,\n",
       "  168,\n",
       "  2489,\n",
       "  167,\n",
       "  117,\n",
       "  174,\n",
       "  19093,\n",
       "  1777,\n",
       "  8745,\n",
       "  1116,\n",
       "  167,\n",
       "  117,\n",
       "  9468,\n",
       "  7301,\n",
       "  20695,\n",
       "  15023,\n",
       "  24632,\n",
       "  1548,\n",
       "  167,\n",
       "  117,\n",
       "  25021,\n",
       "  2660,\n",
       "  21745,\n",
       "  168,\n",
       "  26600,\n",
       "  168,\n",
       "  20497,\n",
       "  12725,\n",
       "  4863,\n",
       "  167,\n",
       "  117,\n",
       "  1126,\n",
       "  20504,\n",
       "  167,\n",
       "  117,\n",
       "  23123,\n",
       "  10024,\n",
       "  1665,\n",
       "  7729,\n",
       "  168,\n",
       "  10558,\n",
       "  167,\n",
       "  117,\n",
       "  24438,\n",
       "  16071,\n",
       "  4043,\n",
       "  3457,\n",
       "  9870,\n",
       "  23179,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  24928,\n",
       "  3818,\n",
       "  12736,\n",
       "  23179,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  5837,\n",
       "  7563,\n",
       "  15622,\n",
       "  5813,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  14991,\n",
       "  1158,\n",
       "  167,\n",
       "  117,\n",
       "  177,\n",
       "  1183,\n",
       "  11439,\n",
       "  5026,\n",
       "  1988,\n",
       "  167,\n",
       "  117,\n",
       "  185,\n",
       "  1348,\n",
       "  18965,\n",
       "  1891,\n",
       "  167,\n",
       "  117,\n",
       "  1137,\n",
       "  1582,\n",
       "  15540,\n",
       "  7698,\n",
       "  168,\n",
       "  177,\n",
       "  1183,\n",
       "  11439,\n",
       "  5026,\n",
       "  1988,\n",
       "  167,\n",
       "  117,\n",
       "  15443,\n",
       "  168,\n",
       "  1126,\n",
       "  10533,\n",
       "  1161,\n",
       "  167,\n",
       "  117,\n",
       "  2633,\n",
       "  168,\n",
       "  14991,\n",
       "  167,\n",
       "  117,\n",
       "  3245,\n",
       "  8005,\n",
       "  1279,\n",
       "  4184,\n",
       "  19911,\n",
       "  1348,\n",
       "  168,\n",
       "  1231,\n",
       "  2087,\n",
       "  24796,\n",
       "  168,\n",
       "  3653,\n",
       "  167,\n",
       "  117,\n",
       "  4267,\n",
       "  1813,\n",
       "  1197,\n",
       "  13836,\n",
       "  167,\n",
       "  117,\n",
       "  185,\n",
       "  5082,\n",
       "  7729,\n",
       "  1361,\n",
       "  167,\n",
       "  117,\n",
       "  14044,\n",
       "  25669,\n",
       "  15391,\n",
       "  1161,\n",
       "  167,\n",
       "  117,\n",
       "  1126,\n",
       "  10712,\n",
       "  15018,\n",
       "  1918,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  22219,\n",
       "  9919,\n",
       "  1708,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  187,\n",
       "  10733,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  4167,\n",
       "  21943,\n",
       "  10721,\n",
       "  113,\n",
       "  9852,\n",
       "  114,\n",
       "  117,\n",
       "  1893,\n",
       "  20955,\n",
       "  1233,\n",
       "  9037,\n",
       "  167,\n",
       "  117,\n",
       "  4091,\n",
       "  168,\n",
       "  20085,\n",
       "  167,\n",
       "  117,\n",
       "  20611,\n",
       "  168,\n",
       "  4152,\n",
       "  167,\n",
       "  117,\n",
       "  2229,\n",
       "  168,\n",
       "  2489,\n",
       "  167,\n",
       "  117,\n",
       "  17963,\n",
       "  168,\n",
       "  5048,\n",
       "  14494,\n",
       "  167,\n",
       "  117,\n",
       "  1107,\n",
       "  27206,\n",
       "  168,\n",
       "  1176,\n",
       "  168,\n",
       "  6946,\n",
       "  167,\n",
       "  117,\n",
       "  1396,\n",
       "  3740,\n",
       "  11466,\n",
       "  167,\n",
       "  117,\n",
       "  10880,\n",
       "  167,\n",
       "  117,\n",
       "  2229,\n",
       "  168,\n",
       "  2489,\n",
       "  167,\n",
       "  117,\n",
       "  177,\n",
       "  24312,\n",
       "  3792,\n",
       "  5053,\n",
       "  3121,\n",
       "  14499,\n",
       "  168,\n",
       "  3943,\n",
       "  167,\n",
       "  117,\n",
       "  1126,\n",
       "  25890,\n",
       "  22948,\n",
       "  8745,\n",
       "  1116,\n",
       "  113,\n",
       "  9852,\n",
       "  114]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a sample batch of token IDs by using the same sequence twice\n",
    "batched_ids = [ids, ids]\n",
    "batched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3a1bb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 2.3330, -1.9936],\n",
      "        [ 2.3330, -1.9936]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_batched_ids = torch.tensor(batched_ids)\n",
    "output_batched = model(input_batched_ids)\n",
    "print(\"Logits:\", output_batched.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "771e19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention masks are used to tell the attention layers (which contextualise each token) in transformer models \n",
    "# to ignore the padding tokens when multiple sequences are of different lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509a509",
   "metadata": {},
   "source": [
    "##### **Some initial thoughts after trying out tokenisation**\n",
    "\n",
    "The overall concept that I'm getting at the moment is that a language model (whether large or small) consists of: \n",
    "\n",
    "* training a data corpus\n",
    "* using tokenizer to encode or decode text data\n",
    "* using pre-trained model of choice as a base or foundation model to train the data provided\n",
    "* producing training output\n",
    "\n",
    "The pre-trained model can be further adjusted or fine-tuned via training the model on smaller high-quality datasets for other more specific NLP tasks.\n",
    "    \n",
    "This means my initial small goal to convert the tensor outputs back into words will actually be the next step after having the training output from a language translation/summarisation/classification model, meaning I'll have to test the trained model on a different set of test data in order to see if the conversion from tensors to strings will make sense (this leads to the latest new plan to try doing NER for the ADRs of tyrosine kinase inhibitors). \n",
    "\n",
    "* possible training workflow of a small part of an early ADR prediction model may be like this: \n",
    "\n",
    "    input training ADR strings (later may add the \"drug\" part) -> encode into token IDs for training -> tensors -> token IDs to be decoded -> ADR strings\n",
    "\n",
    "    code example for the tensors to token IDs to string representations part: \n",
    "    \n",
    "    tokenizer.batch_decode(outputs.context_input_ids, skip_special_tokens=True)\n",
    "\n",
    "* possible workflow to test data in the trained pre-trained ADR decoder model may be like this:\n",
    "\n",
    "    input testing drug-ADRs -> token IDs -> tensors -> token IDs -> ADR strings\n",
    "\n",
    "\n",
    "A useful and informative reference paper to learn about NLP in drug discovery is by Withers et al. - https://doi.org/10.1080/17460441.2025.2490835"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b364f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
