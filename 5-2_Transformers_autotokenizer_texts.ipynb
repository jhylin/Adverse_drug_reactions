{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec3a4fa",
   "metadata": {},
   "source": [
    "*This is currently still a draft which means it may be messy to read!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e436ec",
   "metadata": {},
   "source": [
    "Below is a short demonstration on using AutoTokenizer from [transformers](https://pypi.org/project/transformers/) package to tokenise/encode and decode one line of texts. A separate notebook (5-1_Tokenizers_to_tokenise_texts.ipynb) has been prepared to show how to use tokenizers package to tokenise/encode and decode text data or ADR terms.\n",
    "\n",
    "Reference links: \n",
    "* https://huggingface.co/learn/llm-course/chapter2/4?fw=pt#tokenization\n",
    "* https://huggingface.co/docs/datasets/use_dataset#tokenize-text\n",
    "\n",
    "Plan\n",
    "* initial small goal is to try using tokenizer.decode(), so building a tokenization/tokenizer model first\n",
    "* tokenisation appears to be workable through two different packages e.g. tokenizers or transformers package\n",
    "* the idea is that this text data tokenisation part can be added to a larger DNN model to decode ADRs output later (subject to further idea changes...)\n",
    "\n",
    "* trying HuggingFace's transformers package:\n",
    "1. set up tokenizer model that will tokenize the ADRs/words\n",
    "2. apply tokenizer.decode() function to each tensor row/sequence (via using list comprehension)\n",
    "3. use sample code snippet below to decode tensors: \n",
    "e.g. decoded = [tokenizer.decode(x) for x in adrs_ts]\n",
    "- the code will iterate through each row/sequence of tensors and apply the decode() method \n",
    "which'll transform the numerical IDs back into human-readable texts/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d95e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ] at 2025-05-16 15:47:19.141220\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn.functional import one_hot\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import numpy as np\n",
    "# import datamol as dm\n",
    "# import rdkit\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdFingerprintGenerator\n",
    "# import useful_rdkit_utils as uru\n",
    "import sys, datetime\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# print(f\"Pandas version used: {pd.__version__}\")\n",
    "# print(f\"PyTorch version used: {torch.__version__}\")\n",
    "# print(f\"NumPy version used: {np.__version__}\")\n",
    "#print(f\"RDKit version used: {rdkit.__version__}\")\n",
    "print(f\"Python version used: {sys.version} at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea051933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch example re. saving & reloading tensors\n",
    "# t = torch.tensor([1., 2.])\n",
    "# torch.save(t, 'tensor.pt')\n",
    "# ts = torch.load('tensor.pt')\n",
    "# ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc2e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adrs tensors from 2_ADR_regressor.ipynb after it's saved (from 2_ADR_regressor_save_tensors.ipynb)\n",
    "# adrs_ts = torch.load(\"adr_train_tensors.pt\")\n",
    "# adrs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91201ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# note: some of the pre-trained models are freely available but some of them may be gated \n",
    "# (possibly still freely available but may require signing up a HF account)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f02a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167, 117, 155, 21669, 167, 167, 117, 23123, 8032, 2858, 7939, 168, 10558, 167, 167, 117, 20479, 168, 5099, 168, 10558, 167, 167, 117, 5048, 14494, 167, 167, 117, 1119, 4163, 2941, 168, 172, 3161, 1197, 15342, 1548, 113, 9852, 114, 117, 11911, 168, 4290, 113, 9852, 114, 117, 179, 3984, 12090, 2093, 113, 9852, 114, 117, 188, 27250, 15622, 167, 117, 11850, 1361, 10721, 167, 117, 21447, 168, 22860, 167, 117, 11850, 1361, 168, 22860, 167, 117, 187, 8265, 10721, 167, 117, 1137, 4184, 7111, 5730, 2176, 1348, 168, 2489, 167, 117, 174, 19093, 1777, 8745, 1116, 167, 117, 9468, 7301, 20695, 15023, 24632, 1548, 167, 117, 25021, 2660, 21745, 168, 26600, 168, 20497, 12725, 4863, 167, 117, 1126, 20504, 167, 117, 23123, 10024, 1665, 7729, 168, 10558, 167, 117, 24438, 16071, 4043, 3457, 9870, 23179, 113, 9852, 114, 117, 24928, 3818, 12736, 23179, 113, 9852, 114, 117, 5837, 7563, 15622, 5813, 113, 9852, 114, 117, 14991, 1158, 167, 117, 177, 1183, 11439, 5026, 1988, 167, 117, 185, 1348, 18965, 1891, 167, 117, 1137, 1582, 15540, 7698, 168, 177, 1183, 11439, 5026, 1988, 167, 117, 15443, 168, 1126, 10533, 1161, 167, 117, 2633, 168, 14991, 167, 117, 3245, 8005, 1279, 4184, 19911, 1348, 168, 1231, 2087, 24796, 168, 3653, 167, 117, 4267, 1813, 1197, 13836, 167, 117, 185, 5082, 7729, 1361, 167, 117, 14044, 25669, 15391, 1161, 167, 117, 1126, 10712, 15018, 1918, 113, 9852, 114, 117, 22219, 9919, 1708, 113, 9852, 114, 117, 187, 10733, 113, 9852, 114, 117, 4167, 21943, 10721, 113, 9852, 114, 117, 1893, 20955, 1233, 9037, 167, 117, 4091, 168, 20085, 167, 117, 20611, 168, 4152, 167, 117, 2229, 168, 2489, 167, 117, 17963, 168, 5048, 14494, 167, 117, 1107, 27206, 168, 1176, 168, 6946, 167, 117, 1396, 3740, 11466, 167, 117, 10880, 167, 117, 2229, 168, 2489, 167, 117, 177, 24312, 3792, 5053, 3121, 14499, 168, 3943, 167, 117, 1126, 25890, 22948, 8745, 1116, 113, 9852, 114]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde7d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "# convert_ids_to_tokens() - converts numerical IDs back into corresponding token identifiers\n",
    "token_words = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c19e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abnormal _ LFT ^ ^, headache ^ ^, RTI ^ ^, hemoglobin _ decreased ^ ^, sperm _ count _ decreased ^ ^, edema ^ ^, hepatic _ cirrhosis ( pm ), liver _ failure ( pm ), jaundice ( pm ), syncope ^, sinusitis ^, nasal _ congestion ^, sinus _ congestion ^, rhinitis ^, oropharyngeal _ pain ^, epistaxis ^, nasopharyngitis ^, idiopathic _ pulmonary _ fibrosis ^, anemia ^, hematocrit _ decreased ^, thrombocytopenia ( pm ), neutropenia ( pm ), leukopenia ( pm ), flushing ^, hypotension ^, palpitation ^, orthostatic _ hypotension ^, unstable _ angina ^, hot _ flush ^, gastroesophageal _ reflux _ disease ^, diarrhea ^, pruritus ^, erythema ^, angioedema ( pm ), DRESS ( pm ), rash ( pm ), dermatitis ( pm ), arthralgia ^, joint _ swelling ^, blurred _ vision ^, chest _ pain ^, peripheral _ edema ^, influenza _ like _ illness ^, vertigo ^, fever ^, chest _ pain ^, hypersensitivity _ reaction ^, anaphylaxis ( pm )'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert_tokens_to_string() - merges sub-word tokens into complete words\n",
    "adrs_words = tokenizer.convert_tokens_to_string(tokens)\n",
    "adrs_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c201acd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnormal _ LFT ^ ^, headache ^ ^\n"
     ]
    }
   ],
   "source": [
    "# example to obtain ADR terms from vocabulary indices\n",
    "adrs_terms = tokenizer.decode([22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167])\n",
    "print(adrs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try converting the token ID outputs into torch tensors so they can be used in a pytorch model later\n",
    "# transformers models expect multiple lines of string sequences, so likely need to add tensor dimensions and/or paddings later \n",
    "# may be applicable to one line string sequence or multiple string sequences "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
