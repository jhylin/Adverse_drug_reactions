{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec3a4fa",
   "metadata": {},
   "source": [
    "*This is currently still a draft which means it may be messy to read!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e436ec",
   "metadata": {},
   "source": [
    "Below is a short demonstration on using AutoTokenizer from [transformers](https://pypi.org/project/transformers/) package to tokenise/encode and decode one line of texts. A separate notebook (5-1_Tokenizers_to_tokenise_texts.ipynb) has been prepared to show how to use tokenizers package to tokenise/encode and decode text data or ADR terms.\n",
    "\n",
    "Reference links: \n",
    "* https://huggingface.co/learn/llm-course/chapter2/4?fw=pt#tokenization\n",
    "* https://huggingface.co/docs/datasets/use_dataset#tokenize-text\n",
    "\n",
    "Plan\n",
    "* initial small goal is to try using tokenizer.decode(), so building a tokenization/tokenizer model first\n",
    "* tokenisation appears to be workable through two different packages e.g. tokenizers or transformers package\n",
    "* the idea is that this text data tokenisation part can be added to a larger DNN model to decode ADRs output later (subject to further idea changes... may try a small NER classification model first, see 6_NER_tk_inhibitors.ipynb)\n",
    "\n",
    "* trying HuggingFace's transformers package:\n",
    "1. set up tokenizer model that will tokenize the ADRs/words\n",
    "2. apply tokenizer.decode() function to each tensor row/sequence (via using list comprehension)\n",
    "3. use sample code snippet below to decode tensors: \n",
    "e.g. decoded = [tokenizer.decode(x) for x in adrs_ts]\n",
    "- the code will iterate through each row/sequence of tensors and apply the decode() method \n",
    "which'll transform the numerical IDs back into human-readable texts/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d95e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ] at 2025-05-20 14:24:19.085140\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn.functional import one_hot\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import numpy as np\n",
    "# import datamol as dm\n",
    "# import rdkit\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdFingerprintGenerator\n",
    "# import useful_rdkit_utils as uru\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import sys, datetime\n",
    "\n",
    "# print(f\"Pandas version used: {pd.__version__}\")\n",
    "# print(f\"PyTorch version used: {torch.__version__}\")\n",
    "# print(f\"NumPy version used: {np.__version__}\")\n",
    "#print(f\"RDKit version used: {rdkit.__version__}\")\n",
    "print(f\"Python version used: {sys.version} at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea051933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch example re. saving & reloading tensors\n",
    "# t = torch.tensor([1., 2.])\n",
    "# torch.save(t, 'tensor.pt')\n",
    "# ts = torch.load('tensor.pt')\n",
    "# ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc2e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adrs tensors from 2_ADR_regressor.ipynb after it's saved (from 2_ADR_regressor_save_tensors.ipynb)\n",
    "# adrs_ts = torch.load(\"adr_train_tensors.pt\")\n",
    "# adrs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91201ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "# note: some of the pre-trained models are freely available but some of them may be gated \n",
    "# (possibly still freely available but may require signing up a HF account)\n",
    "# BERT base transformer model (cased -> case-sensitive) has been used - https://huggingface.co/google-bert/bert-base-cased\n",
    "# \"uncased\" version - https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), \" \\\n",
    "\"liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, \" \\\n",
    "\"epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), \" \\\n",
    "\"leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, \" \\\n",
    "\"gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), \" \\\n",
    "\"arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, \" \\\n",
    "\"chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f02a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167, 117, 155, 21669, 167, 167, 117, 23123, 8032, 2858, 7939, 168, 10558, 167, 167, 117, 20479, 168, 5099, 168, 10558, 167, 167, 117, 5048, 14494, 167, 167, 117, 1119, 4163, 2941, 168, 172, 3161, 1197, 15342, 1548, 113, 9852, 114, 117, 11911, 168, 4290, 113, 9852, 114, 117, 179, 3984, 12090, 2093, 113, 9852, 114, 117, 188, 27250, 15622, 167, 117, 11850, 1361, 10721, 167, 117, 21447, 168, 22860, 167, 117, 11850, 1361, 168, 22860, 167, 117, 187, 8265, 10721, 167, 117, 1137, 4184, 7111, 5730, 2176, 1348, 168, 2489, 167, 117, 174, 19093, 1777, 8745, 1116, 167, 117, 9468, 7301, 20695, 15023, 24632, 1548, 167, 117, 25021, 2660, 21745, 168, 26600, 168, 20497, 12725, 4863, 167, 117, 1126, 20504, 167, 117, 23123, 10024, 1665, 7729, 168, 10558, 167, 117, 24438, 16071, 4043, 3457, 9870, 23179, 113, 9852, 114, 117, 24928, 3818, 12736, 23179, 113, 9852, 114, 117, 5837, 7563, 15622, 5813, 113, 9852, 114, 117, 14991, 1158, 167, 117, 177, 1183, 11439, 5026, 1988, 167, 117, 185, 1348, 18965, 1891, 167, 117, 1137, 1582, 15540, 7698, 168, 177, 1183, 11439, 5026, 1988, 167, 117, 15443, 168, 1126, 10533, 1161, 167, 117, 2633, 168, 14991, 167, 117, 3245, 8005, 1279, 4184, 19911, 1348, 168, 1231, 2087, 24796, 168, 3653, 167, 117, 4267, 1813, 1197, 13836, 167, 117, 185, 5082, 7729, 1361, 167, 117, 14044, 25669, 15391, 1161, 167, 117, 1126, 10712, 15018, 1918, 113, 9852, 114, 117, 22219, 9919, 1708, 113, 9852, 114, 117, 187, 10733, 113, 9852, 114, 117, 4167, 21943, 10721, 113, 9852, 114, 117, 1893, 20955, 1233, 9037, 167, 117, 4091, 168, 20085, 167, 117, 20611, 168, 4152, 167, 117, 2229, 168, 2489, 167, 117, 17963, 168, 5048, 14494, 167, 117, 1107, 27206, 168, 1176, 168, 6946, 167, 117, 1396, 3740, 11466, 167, 117, 10880, 167, 117, 2229, 168, 2489, 167, 117, 177, 24312, 3792, 5053, 3121, 14499, 168, 3943, 167, 117, 1126, 25890, 22948, 8745, 1116, 113, 9852, 114]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde7d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', '_', 'L', '##FT', '^', '^', ',', 'headache', '^', '^', ',', 'R', '##TI', '^', '^', ',', 'hem', '##og', '##lo', '##bin', '_', 'decreased', '^', '^', ',', 'sperm', '_', 'count', '_', 'decreased', '^', '^', ',', 'ed', '##ema', '^', '^', ',', 'he', '##pa', '##tic', '_', 'c', '##ir', '##r', '##hos', '##is', '(', 'pm', ')', ',', 'liver', '_', 'failure', '(', 'pm', ')', ',', 'j', '##au', '##ndi', '##ce', '(', 'pm', ')', ',', 's', '##ync', '##ope', '^', ',', 'sin', '##us', '##itis', '^', ',', 'nasal', '_', 'congestion', '^', ',', 'sin', '##us', '_', 'congestion', '^', ',', 'r', '##hin', '##itis', '^', ',', 'or', '##op', '##har', '##yn', '##ge', '##al', '_', 'pain', '^', ',', 'e', '##pis', '##ta', '##xi', '##s', '^', ',', 'na', '##so', '##pha', '##ryn', '##git', '##is', '^', ',', 'id', '##io', '##pathic', '_', 'pulmonary', '_', 'fi', '##bro', '##sis', '^', ',', 'an', '##emia', '^', ',', 'hem', '##ato', '##c', '##rit', '_', 'decreased', '^', ',', 'th', '##rom', '##bo', '##cy', '##top', '##enia', '(', 'pm', ')', ',', 'ne', '##ut', '##rop', '##enia', '(', 'pm', ')', ',', 'le', '##uk', '##ope', '##nia', '(', 'pm', ')', ',', 'flush', '##ing', '^', ',', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'p', '##al', '##pit', '##ation', '^', ',', 'or', '##th', '##ost', '##atic', '_', 'h', '##y', '##pot', '##ens', '##ion', '^', ',', 'unstable', '_', 'an', '##gin', '##a', '^', ',', 'hot', '_', 'flush', '^', ',', 'gas', '##tro', '##es', '##op', '##hage', '##al', '_', 're', '##f', '##lux', '_', 'disease', '^', ',', 'di', '##ar', '##r', '##hea', '^', ',', 'p', '##ru', '##rit', '##us', '^', ',', 'er', '##yt', '##hem', '##a', '^', ',', 'an', '##gio', '##ede', '##ma', '(', 'pm', ')', ',', 'DR', '##ES', '##S', '(', 'pm', ')', ',', 'r', '##ash', '(', 'pm', ')', ',', 'der', '##mat', '##itis', '(', 'pm', ')', ',', 'art', '##hra', '##l', '##gia', '^', ',', 'joint', '_', 'swelling', '^', ',', 'blurred', '_', 'vision', '^', ',', 'chest', '_', 'pain', '^', ',', 'peripheral', '_', 'ed', '##ema', '^', ',', 'in', '##fluenza', '_', 'like', '_', 'illness', '^', ',', 've', '##rt', '##igo', '^', ',', 'fever', '^', ',', 'chest', '_', 'pain', '^', ',', 'h', '##yper', '##sen', '##si', '##ti', '##vity', '_', 'reaction', '^', ',', 'an', '##aph', '##yla', '##xi', '##s', '(', 'pm', ')']\n"
     ]
    }
   ],
   "source": [
    "# convert_ids_to_tokens() - converts numerical IDs back into corresponding token identifiers\n",
    "token_words = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c19e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abnormal _ LFT ^ ^, headache ^ ^, RTI ^ ^, hemoglobin _ decreased ^ ^, sperm _ count _ decreased ^ ^, edema ^ ^, hepatic _ cirrhosis ( pm ), liver _ failure ( pm ), jaundice ( pm ), syncope ^, sinusitis ^, nasal _ congestion ^, sinus _ congestion ^, rhinitis ^, oropharyngeal _ pain ^, epistaxis ^, nasopharyngitis ^, idiopathic _ pulmonary _ fibrosis ^, anemia ^, hematocrit _ decreased ^, thrombocytopenia ( pm ), neutropenia ( pm ), leukopenia ( pm ), flushing ^, hypotension ^, palpitation ^, orthostatic _ hypotension ^, unstable _ angina ^, hot _ flush ^, gastroesophageal _ reflux _ disease ^, diarrhea ^, pruritus ^, erythema ^, angioedema ( pm ), DRESS ( pm ), rash ( pm ), dermatitis ( pm ), arthralgia ^, joint _ swelling ^, blurred _ vision ^, chest _ pain ^, peripheral _ edema ^, influenza _ like _ illness ^, vertigo ^, fever ^, chest _ pain ^, hypersensitivity _ reaction ^, anaphylaxis ( pm )'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert_tokens_to_string() - merges sub-word tokens into complete words\n",
    "adrs_words = tokenizer.convert_tokens_to_string(tokens)\n",
    "adrs_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c201acd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnormal _ LFT ^ ^, headache ^ ^\n"
     ]
    }
   ],
   "source": [
    "# example to obtain ADR terms from vocabulary indices\n",
    "adrs_terms = tokenizer.decode([22832, 168, 149, 26321, 167, 167, 117, 16320, 167, 167])\n",
    "print(adrs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af80ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try converting the token ID outputs into torch tensors so they can be used in a pytorch model later\n",
    "# transformers models expect multiple lines of string sequences, so likely need to add tensor dimensions and/or paddings later \n",
    "# may be applicable to one line string sequence or multiple string sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac593132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 22832,   168,   149, 26321,   167,   167,   117, 16320,   167,\n",
       "           167,   117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,\n",
       "          7939,   168, 10558,   167,   167,   117, 20479,   168,  5099,   168,\n",
       "         10558,   167,   167,   117,  5048, 14494,   167,   167,   117,  1119,\n",
       "          4163,  2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,\n",
       "           114,   117, 11911,   168,  4290,   113,  9852,   114,   117,   179,\n",
       "          3984, 12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,\n",
       "           167,   117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,\n",
       "           167,   117, 11850,  1361,   168, 22860,   167,   117,   187,  8265,\n",
       "         10721,   167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,\n",
       "          2489,   167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,\n",
       "          9468,  7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660,\n",
       "         21745,   168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126,\n",
       "         20504,   167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,\n",
       "           117, 24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,\n",
       "           117, 24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,\n",
       "          7563, 15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,\n",
       "           117,   177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348,\n",
       "         18965,  1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,\n",
       "          1183, 11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,\n",
       "          1161,   167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,\n",
       "          1279,  4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,\n",
       "           167,   117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,\n",
       "          7729,  1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,\n",
       "          1126, 10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,\n",
       "          1708,   113,  9852,   114,   117,   187, 10733,   113,  9852,   114,\n",
       "           117,  4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,\n",
       "          1233,  9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,\n",
       "           168,  4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,\n",
       "           168,  5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,\n",
       "          6946,   167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,\n",
       "           117,  2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,\n",
       "          3121, 14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,\n",
       "          1116,   113,  9852,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API for PreTrainedTokenizerBase class re. parameter on return_tensors \n",
    "# https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__.return_tensors\n",
    "tokenised_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokenised_inputs\n",
    "# output contains \"input_ids\" tensors, \"token_type_ids\" tensors & \"attention_mask\" tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c3d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 22832,   168,   149, 26321,   167,   167,   117, 16320,   167,\n",
      "           167,   117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,\n",
      "          7939,   168, 10558,   167,   167,   117, 20479,   168,  5099,   168,\n",
      "         10558,   167,   167,   117,  5048, 14494,   167,   167,   117,  1119,\n",
      "          4163,  2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,\n",
      "           114,   117, 11911,   168,  4290,   113,  9852,   114,   117,   179,\n",
      "          3984, 12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,\n",
      "           167,   117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,\n",
      "           167,   117, 11850,  1361,   168, 22860,   167,   117,   187,  8265,\n",
      "         10721,   167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,\n",
      "          2489,   167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,\n",
      "          9468,  7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660,\n",
      "         21745,   168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126,\n",
      "         20504,   167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,\n",
      "           117, 24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,\n",
      "           117, 24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,\n",
      "          7563, 15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,\n",
      "           117,   177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348,\n",
      "         18965,  1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,\n",
      "          1183, 11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,\n",
      "          1161,   167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,\n",
      "          1279,  4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,\n",
      "           167,   117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,\n",
      "          7729,  1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,\n",
      "          1126, 10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,\n",
      "          1708,   113,  9852,   114,   117,   187, 10733,   113,  9852,   114,\n",
      "           117,  4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,\n",
      "          1233,  9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,\n",
      "           168,  4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,\n",
      "           168,  5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,\n",
      "          6946,   167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,\n",
      "           117,  2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,\n",
      "          3121, 14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,\n",
      "          1116,   113,  9852,   114,   102]])\n"
     ]
    }
   ],
   "source": [
    "# printing out only the \"input_ids\" tensors\n",
    "print(tokenised_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3eac7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22832,   168,   149, 26321,   167,   167,   117, 16320,   167,   167,\n",
       "          117,   155, 21669,   167,   167,   117, 23123,  8032,  2858,  7939,\n",
       "          168, 10558,   167,   167,   117, 20479,   168,  5099,   168, 10558,\n",
       "          167,   167,   117,  5048, 14494,   167,   167,   117,  1119,  4163,\n",
       "         2941,   168,   172,  3161,  1197, 15342,  1548,   113,  9852,   114,\n",
       "          117, 11911,   168,  4290,   113,  9852,   114,   117,   179,  3984,\n",
       "        12090,  2093,   113,  9852,   114,   117,   188, 27250, 15622,   167,\n",
       "          117, 11850,  1361, 10721,   167,   117, 21447,   168, 22860,   167,\n",
       "          117, 11850,  1361,   168, 22860,   167,   117,   187,  8265, 10721,\n",
       "          167,   117,  1137,  4184,  7111,  5730,  2176,  1348,   168,  2489,\n",
       "          167,   117,   174, 19093,  1777,  8745,  1116,   167,   117,  9468,\n",
       "         7301, 20695, 15023, 24632,  1548,   167,   117, 25021,  2660, 21745,\n",
       "          168, 26600,   168, 20497, 12725,  4863,   167,   117,  1126, 20504,\n",
       "          167,   117, 23123, 10024,  1665,  7729,   168, 10558,   167,   117,\n",
       "        24438, 16071,  4043,  3457,  9870, 23179,   113,  9852,   114,   117,\n",
       "        24928,  3818, 12736, 23179,   113,  9852,   114,   117,  5837,  7563,\n",
       "        15622,  5813,   113,  9852,   114,   117, 14991,  1158,   167,   117,\n",
       "          177,  1183, 11439,  5026,  1988,   167,   117,   185,  1348, 18965,\n",
       "         1891,   167,   117,  1137,  1582, 15540,  7698,   168,   177,  1183,\n",
       "        11439,  5026,  1988,   167,   117, 15443,   168,  1126, 10533,  1161,\n",
       "          167,   117,  2633,   168, 14991,   167,   117,  3245,  8005,  1279,\n",
       "         4184, 19911,  1348,   168,  1231,  2087, 24796,   168,  3653,   167,\n",
       "          117,  4267,  1813,  1197, 13836,   167,   117,   185,  5082,  7729,\n",
       "         1361,   167,   117, 14044, 25669, 15391,  1161,   167,   117,  1126,\n",
       "        10712, 15018,  1918,   113,  9852,   114,   117, 22219,  9919,  1708,\n",
       "          113,  9852,   114,   117,   187, 10733,   113,  9852,   114,   117,\n",
       "         4167, 21943, 10721,   113,  9852,   114,   117,  1893, 20955,  1233,\n",
       "         9037,   167,   117,  4091,   168, 20085,   167,   117, 20611,   168,\n",
       "         4152,   167,   117,  2229,   168,  2489,   167,   117, 17963,   168,\n",
       "         5048, 14494,   167,   117,  1107, 27206,   168,  1176,   168,  6946,\n",
       "          167,   117,  1396,  3740, 11466,   167,   117, 10880,   167,   117,\n",
       "         2229,   168,  2489,   167,   117,   177, 24312,  3792,  5053,  3121,\n",
       "        14499,   168,  3943,   167,   117,  1126, 25890, 22948,  8745,  1116,\n",
       "          113,  9852,   114])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pytorch directly to create tensors from token IDs\n",
    "import torch\n",
    "torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd19b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sample checkpoint & model with the tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Sample multiple sequence data using ADRs of bosentan and carbamazepine\n",
    "sequence = [\"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), \" \\\n",
    "\"liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, \" \\\n",
    "\"epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), \" \\\n",
    "\"neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, \" \\\n",
    "\"hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), \" \\\n",
    "\"dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, \" \\\n",
    "\"vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\", \"constipation^^, leucopenia^^, dizziness^^, \" \\\n",
    "\"sedation^^, ataxia^^, elevated_GGT^^, allergic_skin_reactions^^, eosinophilia^, thrombocytopenia^, neutropenia^, headache^, \" \\\n",
    "\"tremor^, elevated_ALP^, pruritus^, paresthesia^, diplopia^, blurred_vision^, hyponatremia^, fluid_retention^, oedema^, \"\n",
    "\"weight_gain^, reduced_plasma_osmolarity_(ADH_like_effect)^, vertigo^\"]\n",
    "\n",
    "tokens = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# input_ids = torch.tensor(ids)\n",
    "# print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b546c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2043, -2.7013],\n",
       "        [ 2.1903, -1.8928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83e23572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19470,\n",
       "  1035,\n",
       "  1048,\n",
       "  6199,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19387,\n",
       "  2072,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  8649,\n",
       "  4135,\n",
       "  8428,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  18047,\n",
       "  1035,\n",
       "  4175,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  2002,\n",
       "  24952,\n",
       "  2278,\n",
       "  1035,\n",
       "  25022,\n",
       "  12171,\n",
       "  25229,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11290,\n",
       "  1035,\n",
       "  4945,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  14855,\n",
       "  8630,\n",
       "  6610,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  26351,\n",
       "  17635,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  13706,\n",
       "  1034,\n",
       "  1010,\n",
       "  19077,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  1054,\n",
       "  20535,\n",
       "  7315,\n",
       "  1034,\n",
       "  1010,\n",
       "  20298,\n",
       "  21890,\n",
       "  18143,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  4958,\n",
       "  11921,\n",
       "  9048,\n",
       "  2015,\n",
       "  1034,\n",
       "  1010,\n",
       "  17235,\n",
       "  7361,\n",
       "  8167,\n",
       "  6038,\n",
       "  23806,\n",
       "  2483,\n",
       "  1034,\n",
       "  1010,\n",
       "  8909,\n",
       "  3695,\n",
       "  25940,\n",
       "  1035,\n",
       "  21908,\n",
       "  1035,\n",
       "  10882,\n",
       "  12618,\n",
       "  6190,\n",
       "  1034,\n",
       "  1010,\n",
       "  2019,\n",
       "  17577,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  10610,\n",
       "  26775,\n",
       "  4183,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  3393,\n",
       "  6968,\n",
       "  26915,\n",
       "  2401,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23519,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14412,\n",
       "  23270,\n",
       "  3370,\n",
       "  1034,\n",
       "  1010,\n",
       "  2030,\n",
       "  2705,\n",
       "  28696,\n",
       "  4588,\n",
       "  1035,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14480,\n",
       "  1035,\n",
       "  17076,\n",
       "  3981,\n",
       "  1034,\n",
       "  1010,\n",
       "  2980,\n",
       "  1035,\n",
       "  13862,\n",
       "  1034,\n",
       "  1010,\n",
       "  3806,\n",
       "  13181,\n",
       "  2229,\n",
       "  7361,\n",
       "  3270,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  25416,\n",
       "  25148,\n",
       "  1035,\n",
       "  4295,\n",
       "  1034,\n",
       "  1010,\n",
       "  22939,\n",
       "  12171,\n",
       "  20192,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  9413,\n",
       "  26688,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  17076,\n",
       "  3695,\n",
       "  14728,\n",
       "  2863,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4377,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23438,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4315,\n",
       "  18900,\n",
       "  13706,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  2396,\n",
       "  13492,\n",
       "  2140,\n",
       "  10440,\n",
       "  1034,\n",
       "  1010,\n",
       "  4101,\n",
       "  1035,\n",
       "  18348,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  15965,\n",
       "  1035,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1010,\n",
       "  24442,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  7355,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034,\n",
       "  1010,\n",
       "  9016,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  23760,\n",
       "  5054,\n",
       "  28032,\n",
       "  7730,\n",
       "  1035,\n",
       "  4668,\n",
       "  1034,\n",
       "  1010,\n",
       "  9617,\n",
       "  21281,\n",
       "  2721,\n",
       "  9048,\n",
       "  2015,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  9530,\n",
       "  16643,\n",
       "  24952,\n",
       "  2239,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3393,\n",
       "  14194,\n",
       "  26915,\n",
       "  2401,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  4487,\n",
       "  29212,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  7367,\n",
       "  20207,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  29533,\n",
       "  14787,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  1043,\n",
       "  13512,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  27395,\n",
       "  1035,\n",
       "  3096,\n",
       "  1035,\n",
       "  9597,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  1041,\n",
       "  20049,\n",
       "  3630,\n",
       "  21850,\n",
       "  6632,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1010,\n",
       "  27734,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  2632,\n",
       "  2361,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  11968,\n",
       "  25344,\n",
       "  1034,\n",
       "  1010,\n",
       "  16510,\n",
       "  4135,\n",
       "  19312,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  7856,\n",
       "  7913,\n",
       "  10092,\n",
       "  1034,\n",
       "  1010,\n",
       "  8331,\n",
       "  1035,\n",
       "  20125,\n",
       "  1034,\n",
       "  1010,\n",
       "  1051,\n",
       "  14728,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  3635,\n",
       "  1035,\n",
       "  5114,\n",
       "  1034,\n",
       "  1010,\n",
       "  4359,\n",
       "  1035,\n",
       "  12123,\n",
       "  1035,\n",
       "  9808,\n",
       "  5302,\n",
       "  8017,\n",
       "  3012,\n",
       "  1035,\n",
       "  1006,\n",
       "  4748,\n",
       "  2232,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  3466,\n",
       "  1007,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034],\n",
       " [19470,\n",
       "  1035,\n",
       "  1048,\n",
       "  6199,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19387,\n",
       "  2072,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  8649,\n",
       "  4135,\n",
       "  8428,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  18047,\n",
       "  1035,\n",
       "  4175,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  2002,\n",
       "  24952,\n",
       "  2278,\n",
       "  1035,\n",
       "  25022,\n",
       "  12171,\n",
       "  25229,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11290,\n",
       "  1035,\n",
       "  4945,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  14855,\n",
       "  8630,\n",
       "  6610,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  26351,\n",
       "  17635,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  13706,\n",
       "  1034,\n",
       "  1010,\n",
       "  19077,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  8254,\n",
       "  2271,\n",
       "  1035,\n",
       "  20176,\n",
       "  1034,\n",
       "  1010,\n",
       "  1054,\n",
       "  20535,\n",
       "  7315,\n",
       "  1034,\n",
       "  1010,\n",
       "  20298,\n",
       "  21890,\n",
       "  18143,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  4958,\n",
       "  11921,\n",
       "  9048,\n",
       "  2015,\n",
       "  1034,\n",
       "  1010,\n",
       "  17235,\n",
       "  7361,\n",
       "  8167,\n",
       "  6038,\n",
       "  23806,\n",
       "  2483,\n",
       "  1034,\n",
       "  1010,\n",
       "  8909,\n",
       "  3695,\n",
       "  25940,\n",
       "  1035,\n",
       "  21908,\n",
       "  1035,\n",
       "  10882,\n",
       "  12618,\n",
       "  6190,\n",
       "  1034,\n",
       "  1010,\n",
       "  2019,\n",
       "  17577,\n",
       "  1034,\n",
       "  1010,\n",
       "  19610,\n",
       "  10610,\n",
       "  26775,\n",
       "  4183,\n",
       "  1035,\n",
       "  10548,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  3393,\n",
       "  6968,\n",
       "  26915,\n",
       "  2401,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23519,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14412,\n",
       "  23270,\n",
       "  3370,\n",
       "  1034,\n",
       "  1010,\n",
       "  2030,\n",
       "  2705,\n",
       "  28696,\n",
       "  4588,\n",
       "  1035,\n",
       "  1044,\n",
       "  22571,\n",
       "  12184,\n",
       "  3619,\n",
       "  3258,\n",
       "  1034,\n",
       "  1010,\n",
       "  14480,\n",
       "  1035,\n",
       "  17076,\n",
       "  3981,\n",
       "  1034,\n",
       "  1010,\n",
       "  2980,\n",
       "  1035,\n",
       "  13862,\n",
       "  1034,\n",
       "  1010,\n",
       "  3806,\n",
       "  13181,\n",
       "  2229,\n",
       "  7361,\n",
       "  3270,\n",
       "  3351,\n",
       "  2389,\n",
       "  1035,\n",
       "  25416,\n",
       "  25148,\n",
       "  1035,\n",
       "  4295,\n",
       "  1034,\n",
       "  1010,\n",
       "  22939,\n",
       "  12171,\n",
       "  20192,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  9413,\n",
       "  26688,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  17076,\n",
       "  3695,\n",
       "  14728,\n",
       "  2863,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4377,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  23438,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  4315,\n",
       "  18900,\n",
       "  13706,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  1010,\n",
       "  2396,\n",
       "  13492,\n",
       "  2140,\n",
       "  10440,\n",
       "  1034,\n",
       "  1010,\n",
       "  4101,\n",
       "  1035,\n",
       "  18348,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  15965,\n",
       "  1035,\n",
       "  3968,\n",
       "  14545,\n",
       "  1034,\n",
       "  1010,\n",
       "  24442,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  7355,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034,\n",
       "  1010,\n",
       "  9016,\n",
       "  1034,\n",
       "  1010,\n",
       "  3108,\n",
       "  1035,\n",
       "  3255,\n",
       "  1034,\n",
       "  1010,\n",
       "  23760,\n",
       "  5054,\n",
       "  28032,\n",
       "  7730,\n",
       "  1035,\n",
       "  4668,\n",
       "  1034,\n",
       "  1010,\n",
       "  9617,\n",
       "  21281,\n",
       "  2721,\n",
       "  9048,\n",
       "  2015,\n",
       "  1006,\n",
       "  7610,\n",
       "  1007,\n",
       "  9530,\n",
       "  16643,\n",
       "  24952,\n",
       "  2239,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  3393,\n",
       "  14194,\n",
       "  26915,\n",
       "  2401,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  4487,\n",
       "  29212,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  7367,\n",
       "  20207,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  29533,\n",
       "  14787,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  1043,\n",
       "  13512,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  27395,\n",
       "  1035,\n",
       "  3096,\n",
       "  1035,\n",
       "  9597,\n",
       "  1034,\n",
       "  1034,\n",
       "  1010,\n",
       "  1041,\n",
       "  20049,\n",
       "  3630,\n",
       "  21850,\n",
       "  6632,\n",
       "  1034,\n",
       "  1010,\n",
       "  16215,\n",
       "  21716,\n",
       "  5092,\n",
       "  5666,\n",
       "  14399,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  11265,\n",
       "  4904,\n",
       "  18981,\n",
       "  19825,\n",
       "  1034,\n",
       "  1010,\n",
       "  14978,\n",
       "  1034,\n",
       "  1010,\n",
       "  27734,\n",
       "  1034,\n",
       "  1010,\n",
       "  8319,\n",
       "  1035,\n",
       "  2632,\n",
       "  2361,\n",
       "  1034,\n",
       "  1010,\n",
       "  10975,\n",
       "  9496,\n",
       "  5809,\n",
       "  1034,\n",
       "  1010,\n",
       "  11968,\n",
       "  25344,\n",
       "  1034,\n",
       "  1010,\n",
       "  16510,\n",
       "  4135,\n",
       "  19312,\n",
       "  1034,\n",
       "  1010,\n",
       "  18449,\n",
       "  1035,\n",
       "  4432,\n",
       "  1034,\n",
       "  1010,\n",
       "  1044,\n",
       "  22571,\n",
       "  7856,\n",
       "  7913,\n",
       "  10092,\n",
       "  1034,\n",
       "  1010,\n",
       "  8331,\n",
       "  1035,\n",
       "  20125,\n",
       "  1034,\n",
       "  1010,\n",
       "  1051,\n",
       "  14728,\n",
       "  2863,\n",
       "  1034,\n",
       "  1010,\n",
       "  3635,\n",
       "  1035,\n",
       "  5114,\n",
       "  1034,\n",
       "  1010,\n",
       "  4359,\n",
       "  1035,\n",
       "  12123,\n",
       "  1035,\n",
       "  9808,\n",
       "  5302,\n",
       "  8017,\n",
       "  3012,\n",
       "  1035,\n",
       "  1006,\n",
       "  4748,\n",
       "  2232,\n",
       "  1035,\n",
       "  2066,\n",
       "  1035,\n",
       "  3466,\n",
       "  1007,\n",
       "  1034,\n",
       "  1010,\n",
       "  28246,\n",
       "  1034]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a sample batch of token IDs by using the same sequence twice\n",
    "batched_ids = [ids, ids]\n",
    "batched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b22cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 2.1536, -1.9253],\n",
      "        [ 2.1536, -1.9253]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_batched_ids = torch.tensor(batched_ids)\n",
    "output_batched = model(input_batched_ids)\n",
    "print(\"Logits:\", output_batched.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a8bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention masks are used to tell the attention layers (which contextualise each token) in transformer models \n",
    "# to ignore the padding tokens when multiple sequences are of different lengths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
