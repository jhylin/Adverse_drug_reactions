{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d95e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version used: 2.2.3\n",
      "PyTorch version used: 2.2.2\n",
      "NumPy version used: 1.26.4\n",
      "Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "# import datamol as dm\n",
    "# import rdkit\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdFingerprintGenerator\n",
    "# import useful_rdkit_utils as uru\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(f\"Pandas version used: {pd.__version__}\")\n",
    "print(f\"PyTorch version used: {torch.__version__}\")\n",
    "print(f\"NumPy version used: {np.__version__}\")\n",
    "#print(f\"RDKit version used: {rdkit.__version__}\")\n",
    "print(f\"Python version used: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea051933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch example re. saving & reloading tensors\n",
    "t = torch.tensor([1., 2.])\n",
    "torch.save(t, 'tensor.pt')\n",
    "ts = torch.load('tensor.pt')\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc2e5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5256, -0.7502],\n",
       "        [-0.6540, -1.6095],\n",
       "        [-0.1002, -0.6092],\n",
       "        ...,\n",
       "        [ 0.8748,  0.9873],\n",
       "        [-0.7102,  2.8641],\n",
       "        [ 1.1651,  2.0154]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load adrs tensors from 2_ADR_regressor.ipynb after it's saved (from 2_ADR_regressor_save_tensors.ipynb)\n",
    "adrs_ts = torch.load(\"adr_train_tensors.pt\")\n",
    "adrs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e3dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan\n",
    "# If wanting to use tokenizer.decode(), likely may need to build a tokenization/tokenizer model first...?\n",
    "\n",
    "# consider trying HuggingFace's transformers:\n",
    "# 1. Set up tokenizer model that will tokenize the ADRs/words\n",
    "# 2. Apply tokenizer.decode() function to each tensor row/sequence (via using list comprehension)\n",
    "# 3. Use sample code snippet below to decode tensors: \n",
    "# decoded = [tokenizer.decode(x) for x in adrs_ts]\n",
    "# the code will iterate through each row/sequence of tensors and apply the decode() method \n",
    "# which'll transform the numerical IDs back into human-readable texts/words\n",
    "\n",
    "# other methods that may be useful:\n",
    "# convert_ids_to_tokens - converts numerical IDs back into corresponding token identifiers\n",
    "# convert_tokens_to_string - merges sub-word tokens into complete words if needed\n",
    "\n",
    "\n",
    "# may need to also use/integrate code with tokenizers package? (https://pypi.org/project/tokenizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample normalizers code to \"normalise\" texts\n",
    "# somehow the normalizer code is not quite working yet... text data in and the same text data out...\n",
    "\n",
    "# from tokenizers.models import BPE, WordLevel, WordPiece\n",
    "# from tokenizers import Tokenizer, normalizers\n",
    "# from tokenizers.normalizers import StripAccents, Sequence, Replace\n",
    "\n",
    "# BPE - byte pair encoding\n",
    "# bpe_tokenizer = Tokenizer(BPE())\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "# bpe_tokenizer.normalizer = normalizers.Sequence([StripAccents()])\n",
    "## normalizer seems to be set already even though code seems not right within the normalizers.Sequence() (?)\n",
    "# print(bpe_tokenizer.normalizer)\n",
    "\n",
    "# sentences = ['abdominal_pain', 'Höw aRę ŸõŪ dÔįñg?']\n",
    "\n",
    "# normalized_senences = [bpe_tokenizer.normalizer.normalize_str(s) for s in sentences]\n",
    "# normalized_senences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f9b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hepatic_cirrhosis(pm),': 20,\n",
       " 'leukopenia(pm),': 28,\n",
       " 'orthostatic_hypotension^,': 34,\n",
       " 'vertigo^,': 46,\n",
       " 'dermatitis(pm),': 9,\n",
       " 'hot_flush^,': 21,\n",
       " 'liver_failure(pm),': 29,\n",
       " 'angioedema(pm),': 6,\n",
       " 'blurred_vision^,': 8,\n",
       " 'hypersensitivity_reaction^,': 22,\n",
       " 'abnormal_LFT^^,': 3,\n",
       " 'flushing^,': 15,\n",
       " 'joint_swelling^,': 27,\n",
       " 'unstable_angina^,': 45,\n",
       " 'thrombocytopenia(pm),': 44,\n",
       " 'epistaxis^,': 12,\n",
       " 'hypotension^,': 23,\n",
       " 'palpitation^,': 35,\n",
       " 'oropharyngeal_pain^,': 33,\n",
       " 'diarrhea^,': 10,\n",
       " 'sinus_congestion^,': 40,\n",
       " 'erythema^,': 13,\n",
       " 'nasal_congestion^,': 30,\n",
       " 'chest_pain^,': 0,\n",
       " 'hematocrit_decreased^,': 18,\n",
       " 'rash(pm),': 38,\n",
       " 'rhinitis^,': 39,\n",
       " 'headache^^,': 17,\n",
       " 'RTI^^,': 2,\n",
       " 'jaundice(pm),': 26,\n",
       " 'anaphylaxis(pm)': 4,\n",
       " 'influenza_like_illness^,': 25,\n",
       " 'peripheral_edema^,': 36,\n",
       " 'syncope^,': 43,\n",
       " 'nasopharyngitis^,': 31,\n",
       " 'pruritus^,': 37,\n",
       " 'anemia^,': 5,\n",
       " 'arthralgia^,': 7,\n",
       " 'sinusitis^,': 41,\n",
       " 'gastroesophageal_reflux_disease^,': 16,\n",
       " 'edema^^,': 11,\n",
       " 'sperm_count_decreased^^,': 42,\n",
       " 'idiopathic_pulmonary_fibrosis^,': 24,\n",
       " 'DRESS(pm),': 1,\n",
       " 'fever^,': 14,\n",
       " 'hemoglobin_decreased^^,': 19,\n",
       " 'neutropenia(pm),': 32}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text data from one of CYP3A4 substrates - bosenten's ADRs \n",
    "# since ADRs data are preprocessed a bit more than raw texts found elsewhere, decided to go straight to create a tokenizer\n",
    "data = [\"abnormal_LFT^^, headache^^, RTI^^, hemoglobin_decreased^^, sperm_count_decreased^^, edema^^, hepatic_cirrhosis(pm), liver_failure(pm), jaundice(pm), syncope^, sinusitis^, nasal_congestion^, sinus_congestion^, rhinitis^, oropharyngeal_pain^, epistaxis^, nasopharyngitis^, idiopathic_pulmonary_fibrosis^, anemia^, hematocrit_decreased^, thrombocytopenia(pm), neutropenia(pm), leukopenia(pm), flushing^, hypotension^, palpitation^, orthostatic_hypotension^, unstable_angina^, hot_flush^, gastroesophageal_reflux_disease^, diarrhea^, pruritus^, erythema^, angioedema(pm), DRESS(pm), rash(pm), dermatitis(pm), arthralgia^, joint_swelling^, blurred_vision^, chest_pain^, peripheral_edema^, influenza_like_illness^, vertigo^, fever^, chest_pain^, hypersensitivity_reaction^, anaphylaxis(pm)\"]\n",
    "\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "\n",
    "# have not yet taken into account of unknown words or padding token\n",
    "tokenizer = Tokenizer(models.WordLevel())\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=100000)\n",
    "\n",
    "# training tokenizer \n",
    "# specify iterator - pass through iterator a sequence of sequences in the data via using map() function to apply split()\n",
    "# and trainer\n",
    "tokenizer.train_from_iterator(map(lambda x: x.split(), data), trainer=trainer)\n",
    "\n",
    "tokenizer.get_vocab()\n",
    "# returns the indices of each token in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b1612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal_LFT^^,', 'headache^^,', 'RTI^^,', 'hemoglobin_decreased^^,', 'sperm_count_decreased^^,', 'edema^^,', 'hepatic_cirrhosis(pm),', 'liver_failure(pm),', 'jaundice(pm),', 'syncope^,', 'sinusitis^,', 'nasal_congestion^,', 'sinus_congestion^,', 'rhinitis^,', 'oropharyngeal_pain^,', 'epistaxis^,', 'nasopharyngitis^,', 'idiopathic_pulmonary_fibrosis^,', 'anemia^,', 'hematocrit_decreased^,', 'thrombocytopenia(pm),', 'neutropenia(pm),', 'leukopenia(pm),', 'flushing^,', 'hypotension^,', 'palpitation^,', 'orthostatic_hypotension^,', 'unstable_angina^,', 'hot_flush^,', 'gastroesophageal_reflux_disease^,', 'diarrhea^,', 'pruritus^,', 'erythema^,', 'angioedema(pm),', 'DRESS(pm),', 'rash(pm),', 'dermatitis(pm),', 'arthralgia^,', 'joint_swelling^,', 'blurred_vision^,', 'chest_pain^,', 'peripheral_edema^,', 'influenza_like_illness^,', 'vertigo^,', 'fever^,', 'chest_pain^,', 'hypersensitivity_reaction^,', 'anaphylaxis(pm)']\n"
     ]
    }
   ],
   "source": [
    "# using str.split() but punctuations such as commas are not stripped/splitted\n",
    "for t in data:\n",
    "    print(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f381bfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('abnormal_LFT', (0, 12)),\n",
       "  ('^^,', (12, 15)),\n",
       "  ('headache', (16, 24)),\n",
       "  ('^^,', (24, 27)),\n",
       "  ('RTI', (28, 31)),\n",
       "  ('^^,', (31, 34)),\n",
       "  ('hemoglobin_decreased', (35, 55)),\n",
       "  ('^^,', (55, 58)),\n",
       "  ('sperm_count_decreased', (59, 80)),\n",
       "  ('^^,', (80, 83)),\n",
       "  ('edema', (84, 89)),\n",
       "  ('^^,', (89, 92)),\n",
       "  ('hepatic_cirrhosis', (93, 110)),\n",
       "  ('(', (110, 111)),\n",
       "  ('pm', (111, 113)),\n",
       "  ('),', (113, 115)),\n",
       "  ('liver_failure', (116, 129)),\n",
       "  ('(', (129, 130)),\n",
       "  ('pm', (130, 132)),\n",
       "  ('),', (132, 134)),\n",
       "  ('jaundice', (135, 143)),\n",
       "  ('(', (143, 144)),\n",
       "  ('pm', (144, 146)),\n",
       "  ('),', (146, 148)),\n",
       "  ('syncope', (149, 156)),\n",
       "  ('^,', (156, 158)),\n",
       "  ('sinusitis', (159, 168)),\n",
       "  ('^,', (168, 170)),\n",
       "  ('nasal_congestion', (171, 187)),\n",
       "  ('^,', (187, 189)),\n",
       "  ('sinus_congestion', (190, 206)),\n",
       "  ('^,', (206, 208)),\n",
       "  ('rhinitis', (209, 217)),\n",
       "  ('^,', (217, 219)),\n",
       "  ('oropharyngeal_pain', (220, 238)),\n",
       "  ('^,', (238, 240)),\n",
       "  ('epistaxis', (241, 250)),\n",
       "  ('^,', (250, 252)),\n",
       "  ('nasopharyngitis', (253, 268)),\n",
       "  ('^,', (268, 270)),\n",
       "  ('idiopathic_pulmonary_fibrosis', (271, 300)),\n",
       "  ('^,', (300, 302)),\n",
       "  ('anemia', (303, 309)),\n",
       "  ('^,', (309, 311)),\n",
       "  ('hematocrit_decreased', (312, 332)),\n",
       "  ('^,', (332, 334)),\n",
       "  ('thrombocytopenia', (335, 351)),\n",
       "  ('(', (351, 352)),\n",
       "  ('pm', (352, 354)),\n",
       "  ('),', (354, 356)),\n",
       "  ('neutropenia', (357, 368)),\n",
       "  ('(', (368, 369)),\n",
       "  ('pm', (369, 371)),\n",
       "  ('),', (371, 373)),\n",
       "  ('leukopenia', (374, 384)),\n",
       "  ('(', (384, 385)),\n",
       "  ('pm', (385, 387)),\n",
       "  ('),', (387, 389)),\n",
       "  ('flushing', (390, 398)),\n",
       "  ('^,', (398, 400)),\n",
       "  ('hypotension', (401, 412)),\n",
       "  ('^,', (412, 414)),\n",
       "  ('palpitation', (415, 426)),\n",
       "  ('^,', (426, 428)),\n",
       "  ('orthostatic_hypotension', (429, 452)),\n",
       "  ('^,', (452, 454)),\n",
       "  ('unstable_angina', (455, 470)),\n",
       "  ('^,', (470, 472)),\n",
       "  ('hot_flush', (473, 482)),\n",
       "  ('^,', (482, 484)),\n",
       "  ('gastroesophageal_reflux_disease', (485, 516)),\n",
       "  ('^,', (516, 518)),\n",
       "  ('diarrhea', (519, 527)),\n",
       "  ('^,', (527, 529)),\n",
       "  ('pruritus', (530, 538)),\n",
       "  ('^,', (538, 540)),\n",
       "  ('erythema', (541, 549)),\n",
       "  ('^,', (549, 551)),\n",
       "  ('angioedema', (552, 562)),\n",
       "  ('(', (562, 563)),\n",
       "  ('pm', (563, 565)),\n",
       "  ('),', (565, 567)),\n",
       "  ('DRESS', (568, 573)),\n",
       "  ('(', (573, 574)),\n",
       "  ('pm', (574, 576)),\n",
       "  ('),', (576, 578)),\n",
       "  ('rash', (579, 583)),\n",
       "  ('(', (583, 584)),\n",
       "  ('pm', (584, 586)),\n",
       "  ('),', (586, 588)),\n",
       "  ('dermatitis', (589, 599)),\n",
       "  ('(', (599, 600)),\n",
       "  ('pm', (600, 602)),\n",
       "  ('),', (602, 604)),\n",
       "  ('arthralgia', (605, 615)),\n",
       "  ('^,', (615, 617)),\n",
       "  ('joint_swelling', (618, 632)),\n",
       "  ('^,', (632, 634)),\n",
       "  ('blurred_vision', (635, 649)),\n",
       "  ('^,', (649, 651)),\n",
       "  ('chest_pain', (652, 662)),\n",
       "  ('^,', (662, 664)),\n",
       "  ('peripheral_edema', (665, 681)),\n",
       "  ('^,', (681, 683)),\n",
       "  ('influenza_like_illness', (684, 706)),\n",
       "  ('^,', (706, 708)),\n",
       "  ('vertigo', (709, 716)),\n",
       "  ('^,', (716, 718)),\n",
       "  ('fever', (719, 724)),\n",
       "  ('^,', (724, 726)),\n",
       "  ('chest_pain', (727, 737)),\n",
       "  ('^,', (737, 739)),\n",
       "  ('hypersensitivity_reaction', (740, 765)),\n",
       "  ('^,', (765, 767)),\n",
       "  ('anaphylaxis', (768, 779)),\n",
       "  ('(', (779, 780)),\n",
       "  ('pm', (780, 782)),\n",
       "  (')', (782, 783))]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pre_tokenizer will split at white spaces and remove punctuations, and set tokens for each word and each punctuation\n",
    "pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "split_data = [pre_tokenizer.pre_tokenize_str(t) for t in data]\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a15731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, token: chest_pain^,\n",
      "ID: 1, token: DRESS(pm),\n",
      "ID: 2, token: RTI^^,\n",
      "ID: 3, token: abnormal_LFT^^,\n",
      "ID: 4, token: anaphylaxis(pm)\n",
      "ID: 5, token: anemia^,\n",
      "ID: 6, token: angioedema(pm),\n",
      "ID: 7, token: arthralgia^,\n",
      "ID: 8, token: blurred_vision^,\n",
      "ID: 9, token: dermatitis(pm),\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'ID: {i}, token: {tokenizer.id_to_token(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique tokens (words)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e6d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
